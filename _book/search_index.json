[["index.html", "Ciencia de Datos para Activismo Jurídico Una introducción a la exploración, análisis y visualización de datos para activistas ¿Para quién es esto? Antes de empezar", " Ciencia de Datos para Activismo Jurídico Una introducción a la exploración, análisis y visualización de datos para activistas Antonio Vazquez Brust Demian Zayat ¿Para quién es esto? Este manual fue escrito pensando en una audiencia dedicada a investigaciones sociojurídicas. Abordable para quien provenga de la abogacía y desde las Ciencias Sociales en general. Aún así, y por supuesto, todas las personas y algoritmos con capacidad de procesar lenguaje son bienvenidas. Esperamos que el tono introductorio del texto, así como el esfuerzo puesto en explicar los conceptos con la mayor simplicidad posible, resulten de interés para un público amplio. No hace falta ningún conocimiento previo de programación; todas las herramientas necesarias serán explicadas sobre la marcha. Antes de empezar Para practicar los ejemplos que se explicarán a lo largo del libro, es necesario instalar el lenguaje de programación R, y la interfaz gráfica RStudio Desktop. "],["qué-es-la-ciencia-de-datos.html", "1 ¿Qué es la ciencia de datos? 1.1 ¿Qué significa hacer ciencia de datos?", " 1 ¿Qué es la ciencia de datos? La Big Data ha llegado para quedarse, y asumimos que su efecto en la sociedad será permanente. Así como pasó con la escritura, los medios de comunicación o tantos otros inventos humanos de inmenso impacto cultural, el incremento en la producción y análisis computacional de grandes volúmenes de datos está transformando cada una de nuestras actividades. Algunas profesiones se ven en crisis, otras se benefician, y también se crean algunas nuevas. Big data es un término impreciso, que se usa cuando queremos hablar de los datos que nuestra sociedad crea y procesa en forma digital, con cada vez más creciente velocidad, volumen, y variedad. En forma acorde, data scientist o “científico de datos” es también una profesión, o una actividad, que aún no está definida con toda claridad. El término, que abarca a quienes en forma cotidiana aplican técnicas de programación para analizar datos, no existía antes del 2008. Sólo cuatro años después la publicación Harvard Business Review agitó las aguas al declarar a la científicos de datos como la profesión “más sexy del siglo XXI”1. Títulos exagerados aparte2, lo que es seguro es que la disciplina ofrece un conjunto cada vez más maduro de saberes orientados a explotar datos para extraer conocimiento. Las técnicas y principios que la comunidad de la ciencia de datos ha desarrollado pueden ser aprovechados en muchos ámbitos. Entre ellos, el de las ciencias sociales, que también están en una etapa de transformación e incorporan la programación analítica como un recurso cada vez extendido. Avanzar las fronteras de la ciencia de datos, crear los algoritmos y técnicas informáticas que abren nuevas posibilidades de análisis es una tarea compleja, llevada a cabo por especialistas con profundos conocimientos de matemática. Y sin embargo “usar” la ciencia de datos, aplicar sus principios para resolver problemas complejos, es bastante más fácil. Para empezar sólo necesitamos paciencia para aprender algunos conceptos fundamentales de programación y estadística, empleándolos para entender y comunicar con datos. De eso se trata este libro. 1.1 ¿Qué significa hacer ciencia de datos? Ya dijimos que la ciencia de datos se trata de emplear técnicas de programación para analizar datos. Pero no es sólo eso; la ciencia de datos aplicada requiere el desarrollo de habilidades en cuatro áreas: Programación. Según la definición que hemos aceptado, todo científico de datos utiliza la programación para explicar a las computadoras lo que necesita de ellas. Al hacerlo, emplea el “pensamiento computacional”: la habilidad de reducir una tarea compleja a una serie de pasos que pueden resolverse con código interpretado por una computadora. Aclaremos por si hiciera falta que no todos los problemas son solubles por medios computacionales, pero muchos lo son, al menos en parte. El científico de datos pone en práctica algunas técnicas de programación (o muchas, según el grado de especialización) para resolver problemas que sería impráctico abordar de otro modo. Estadística. ¡Inescapable! También poderosa, a veces anti-intuitiva, cuando tenemos suerte reveladora. La estadística es muchas cosas, pero -a pesar de su mala fama- aburrida jamás. Sólo es cuestión de amigarse con ella. Vamos a necesitarla para extraer conocimiento de los datos. Es sorprendente lo mucho que puede lograrse con sólo unos rudimentos (media, mediana, desvío estándar y cuartiles) y de allí en más sólo es cuestión de profundizar paso a paso. Comunicación. Un científico de datos combina habilidades “duras” con otras que requieren empatizar con los demás: las que se relacionan con la comunicación y la colaboración interdisciplinaria. Encontrar la forma de explicar procesos complejos, de llevar las revelaciones de un modelo estadístico a términos que tengan sentido para un público amplio, crear visualizaciones que permitan a terceros “leer” los datos y sacar conclusiones por su cuenta. Parte de hacer ciencia de datos es saber cómo discutir los datos usados y los resultados obtenidos con un interlocutores muy diversos: audiencia general, funcionarios públicos, colegas, especialistas de otras disciplinas, etcétera. Conocimiento de dominio. El conocimiento de dominio es la experiencia acumulada en un campo particular de actividad humana: agricultura, relaciones públicas, física cuántica, crianza de niños. Complementa de forma imprescindible a las habilidades analíticas. El conocimiento de dominio no sólo ayuda a discernir si las respuestas obtenidas mediante un sofisticado análisis estadístico tienen sentido. También es necesario para saber cuáles son las preguntas que deberíamos estar haciendo. Las cuatros habilidades entran en acción en cada proyecto que involucra ciencia de datos, en mayor o menor medida de acuerdo a la etapa de análisis. Hablando de etapas, Hadley Wickham, uno de los referentes actuales en el campo, las define así: Figura 1.1: etapas en la aplicación de ciencia de datos Y todo ello llevado a cabo mediante la programación, por supuesto. A lo largo de los capítulos de este libro vamos a aprender técnicas de programación que nos permitan atravesar cada uno de los pasos del proceso, y al hacerlo estaremos ejercitando las cuatro habilidades que involucra la ciencia de datos. Allá vamos. Véase “Data Scientist: The Sexiest Job of the 21st Century”, https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century↩︎ Sólo un par de años después llegó la confesión: “The Sexiest Job of the 21st Century is Tedious, and that Needs to Change”, https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century↩︎ "],["una-presentación-a-toda-marcha-de-r.html", "2 Una presentación a toda marcha de R 2.1 Nuestro primer proyecto en R 2.2 Visualización: la exploración gráfica de la información 2.3 El veredicto final 2.4 Ejercicios", " 2 Una presentación a toda marcha de R R es un lenguaje de programación especializado en análisis y visualización de datos. Es un producto de código abierto, lo cual significa que cualquier persona puede usarlo y modificarlo sin pagar licencias ni costos de adquisición de ningún tipo. Expertos de todo el mundo colaboran en forma activa con el proyecto, no sólo desarrollando el lenguaje en sí (llamado “R base”), sino también extendiéndolo con nuevas habilidades que pueden ser incorporadas por los usuarios finales en forma de “paquetes” instalables. La calidad del lenguaje en sí, de los paquetes instalables que le agregan un sinfín de funciones (desde algoritmos de inteligencia artificial hasta mapas interactivos) y de la comunidad de usuarios que comparte información en foros y blogs, ha hecho de R uno de los lenguajes de programación más populares del mundo. En el campo del análisis de datos, es la herramienta por excelencia en muchas universidades, empresas de tecnología, y redacciones de periodismo de datos. 2.1 Nuestro primer proyecto en R A continuación reproduciremos un ejercicio paso a paso, para ilustrar la potencia de una herramienta de análisis como R. Que nadie se preocupe si algunas de las operaciones parecen no tener sentido, o resultan arbitrarias. ¡Es normal! Nadie aprende un lenguaje en 10 minutos, sea R o esperanto. La idea es tener exposición temprana a un caso de uso interesante, usando datos reales. Y que nos sirva como motivación para practicar luego ejercicios básicos que son muy necesarios pero, a veces, no tan emocionantes. 2.1.1 A investigar: ¿Cual es la diferencia en mortalidad infantil entre el sur y el norte de la Ciudad Autónoma de Buenos Aires? Buenos Aires es una ciudad que desde hace décadas presenta una marcada polarización entre sus barrios del sur, relativamente menos desarrollados, y los del norte donde el nivel socioeconómico y la calidad de vida son mayores. Figura 2.1: Artículo en la edición online de El País Uno de los aspectos más lamentables de la disparidad norte-sur, y sin duda de los que más polémica y acusaciones cruzadas ha generado, es la diferencia en la tasa de mortalidad infantil de acuerdo a la región de la ciudad. ¿Qué tan grande es esa diferencia? ¿Cómo se distribuye geográficamente? Vamos a utilizar R para responder esas preguntas y visualizar los resultados de nuestro análisis, utilizando como fuente cifras oficiales publicada por la ciudad. 2.1.2 Crear un proyecto en RStudio El primer paso es ejecutar RStudio, que ya deberíamos tener disponible en nuestro sistema. Una vez abierta la interfaz gráfica, creamos un proyecto nuevo, cliqueando en File -&gt; New Project... -&gt; New Directory -&gt; New Project. En la ventana que surge, elegir un nombre para el proyecto (por ejemplo, “Practicando R”) y finalizar la operación cliqueando en Create project. Utilizar proyectos nos permite continuar otro día desde donde dejamos la tarea al terminar una sesión. Es sólo cuestión de recuperar el proyecto deseado la próxima vez que abrimos RStudio, cliqueando en File -&gt; Recent Projects -&gt; \"nombre de mi proyecto\". Por ahora, sigamos trabajando. Vamos a crear un “script”. Un script, como su nombre en inglés lo indica, es un guión; una serie de pasos que escribimos para que nuestra computadora ejecute en secuencia. Cliqueamos en File -&gt; New File -&gt; R Script. De inmediato se abre una ventana con un editor de texto. ¡Ahora empieza la acción! 2.1.3 Escribiendo un script Aprovechemos para dar un nombre a los áreas que vemos en RStudio: Figura 2.2: La interfaz de RStudio Vamos a escribir nuestro código (las instrucciones que R entiende) en el panel de edición. Los resultados van a aparecer en la consola (cuando se trate de texto) o en el panel de salida (cuando produzcamos gráficos) Por ejemplo, podemos escribir el panel de edición la instrucción para mostrar el resultado de una operación matemático: sqrt(144) sqrt() es una función. En el mundo de la programación, las funciones son secuencias de código ya listas para usar, que realizan tareas útiles. Por ejemplo, mostrar algo en pantalla. En nuestro caso, completamos la función con algo más: un parámetro, pues así se le llama a los valores que una función espera de parte del usuario para saber que hacer. La función sqrt() espera que le demos un número para el cual calcular su raíz cuadrada (square root en inglés), y eso hicimos: le pasamos cómo parámetro 144, un número. Los parámetros siempre se escriben entre paréntesis, a continuación del nombre de la función. Ahora vamos a aprender la combinación de teclas más importante al usar RStudio: Ctrl + Enter. Presionar Ctrl + Enter al terminar de escribir una instrucción hace que RStudio la ejecute de inmediato, y espere en la siguiente instrucción, si la hubiera. También podemos buscar una línea que deseemos ejecutar, posicionando el cursor de texto (que luce como una barra vertical que titila, en el panel de edición) sobre ella. Si a continuación pulsamos Ctrl + Enter, la línea será ejecutada y el cursor se moverá sólo hasta la siguiente línea, listo para repetir el proceso. La modalidad de ejecución línea por línea es muy útil para lo que se llama “análisis interactivo”. Uno ejecuta un comando, observa el resultado, y en base a eso decide su próxima acción: cambiar parámetros e intentarlo de nuevo, dar por buenos los resultados y usarlos para una tarea subsiguiente… etc. Por ejemplo, si escribimos las siguientes líneas: sqrt(144) mensaje &lt;- &quot;Hola mundo&quot; mensaje …y posicionamos el cursor en cualquier posición de la primera línea, para luego pulsar Ctrl + Enter tres veces, veremos que las instrucciones son ejecutadas línea a línea. sqrt(144) ## [1] 12 mensaje &lt;- &quot;Hola mundo&quot; mensaje ## [1] &quot;Hola mundo&quot; Dos de ellas (la primera y la última) mostraron una salida en pantalla, y la del medio, no. Esto es porque algunas operaciones entregan algo como resultado -un número, un texto, un gráfico, u otros tipos de salida que ya veremos- mientras que otras hacen su tarea silenciosamente sin expresar nada. En este caso, la operación silenciosa fue la de asignación: mensaje &lt;- \"Hola mundo\" es una instrucción que le pide a R que cree una variable llamada “mensaje” (o que la encuentre si ya existe) y que le asigne como valor el texto “Hola mundo”. ¿Cómo sabemos que la instrucción se llevó a cabo, a pesar de no producir una salida? En general, es un tema de confianza. Si una instrucción no genera un mensaje de error, si es silenciosa, se asume que pudo cumplir su cometido. En este caso, además lo hemos verificado. La línea final, mensaje pide a R que busque la variable, y muestre en pantalla su contenido (esa es una característica muy práctica del lenguaje: para saber el contenido de una variable, basta con escribirla y ejecutar la línea). Y al hacerlo, comprobamos que la variable contiene precisamente lo que hemos tipeado. De paso, hay que mencionar que la creación y manipulación de variables es un concepto clave en programación. Trabajar con variables nos permite almacenar valores para usarlos después, además de hacer nuestro código más fácil de leer y compartir con otros, en especial cuando usamos nombres de variable auto-explicativos. Como ejemplo de ésto ultimo comparemos x &lt;- 8 * 6 x ## [1] 48 … con ancho_habitacion_m &lt;- 8 profundiad_habitacion_m &lt;- 6 superficie_habitacion_m2 &lt;- ancho_habitacion_m * profundiad_habitacion_m superficie_habitacion_m2 ## [1] 48 En su resultado ambas expresiones son iguales, dado que producen lo mismo. Pero la segunda esta escrita de una forma mucho más clara para un ser humano, que hace más fácil interpretar su lógica… ¡está calculando la superficie en metros cuadrados de una habitación!. Es muy importante escribir nuestro código de la forma más explícita posible, aunque requiera tipear un poco más. Con ello, le hacemos la vida más fácil a otras personas que interpreten nuestros programas. Y también a nosotros mismos en el futuro, cuando debamos lidiar con un programa que escribimos tiempo atrás y del que a duras penas recordamos su lógica. A todo esto… ¿no se suponía que íbamos a investigar la mortalidad infantil en la Ciudad de Buenos Aires?. Suficiente introducción… ¡allá vamos! 2.1.4 Cargar los datos Vamos a cargar datos de mortalidad infantil, por comuna de la ciudad, en el año 2016, publicados por la Dirección General de Estadística y Censos de Buenos Aires. El formato original de los datos es “.xls” (planilla de hojas de cálculo). Yo lo he convertido a .csv (“comma separated values”) un formato muy popular en el mundo de la ciencia de datos, ya que es muy fácil de manipular y compartir entre sistemas… es posible abrir un archivo .csv hasta con el humilde block de notas. Al igual que los archivos .xls, los .csv se utilizan para guardar información tabular: un rectángulo con filas y columnas. R incluye una función que lee archivos .csv, que se llama read.csv. La usamos así: mortalidad &lt;- read.csv(&#39;https://bitsandbricks.github.io/data/mortalidad_infantil_caba_2016.csv&#39;) Obsérvese que los datos están alojados en un servidor de internet (accesibles vía una dirección web). Eso no es problema para la función read.csv(), que con la misma soltura lee archivos guardados en nuestra PC o publicados en un sitio online. Una vez leído el archivo, para ver el contenido de la variable donde guardamos el resultado -que hemos llamado mortalidad- sólo hace falta escribir su nombre: mortalidad Vemos que la tabla tiene 15 filas (una por cada comuna de la ciudad) y 2 columnas (una que indica la comuna, y otra con el valor de mortalidad infantil para el año 2016). Sobre dataframes y datasets: En R, las tablas son llamadas dataframes. El dataframe es el objeto por excelencia del análisis de datos. En concepto, es muy similar a una tabla de Excel; ambos formatos guardan información en celdas identificadas por fila y columna. El dataframe es una estructura de datos digitales en forma de tabla. Es probable que nos crucemos con un término similar: dataset (o “data set”). Este otro anglicismo se refiere a cualquier colección de datos, también típicamente en formato de tabla, que se publica o comparte. Por ejemplo si un organismo de gobierno publica sus gastos mensuales en internet, en cualquier formato fácil de descargar y abrir, decimos que ofrece “un dataset de gastos”. Algunas funciones útiles para explorar un dataframe son dim(), que nos da las dimensiones del dataframe (cantidad de filas y columnas), names() que nos dice como se llaman sus columnas (que en general representan variables), y head() que nos permite echar un vistazo rápido al contenido, mostrando sólo las seis primeras filas (ésto es útil porque con frecuencia trabajamos con dataframes que contienen miles o millones de filas, por lo que no tiene sentido tratar de volcar todas en pantalla). dim(mortalidad) ## [1] 15 2 names(mortalidad) ## [1] &quot;Comuna&quot; &quot;Tasa2016&quot; head(mortalidad) 2.2 Visualización: la exploración gráfica de la información Ahora es vamos a pisar el acelerador. Insisto: nadie debe preocuparse si algunos conceptos parecen ser demasiado complejos. En las próximas secciones practicaremos de forma gradual las técnicas que vamos a usar ahora, y todo tendrá sentido -¡lo prometo!. Pero antes, seamos un poquito irresponsables con el poder de R y empleemos un arsenal sofisticado de herramientas para ver de que somos capaces. En la introducción hablamos de los paquetes, conjuntos de programas que extienden la funcionalidad de R. Vamos a cargar uno de los paquetes más usados, tidyverse. Tidyverse incluye una gran cantidad de funciones diseñadas por y para practicantes de la ciencia de datos. Estas funciones comparten una filosofía y una sintaxis común, por lo que al aprender una en cierto modo aprendemos a usar todas. El valor que aportan es que, sin dudas, ayudan a realizar de manera más fácil las tareas típicas de la ciencia de datos: importar, limpiar, comprender y comunicar datos. Si acabamos de instalar R y RStudio, el paquete aún no estará disponible en nuestro sistema. Para instalarlo, usamos la función install.packages() y le pasamos el nombre del paquete deseado, “tidyverse”, entre comillas. install.packages(&quot;tidyverse&quot;) De aquí en más, podremos activar el conjunto de funciones que provee tidyverse cada vez que queramos. Para eso, lo invocamos con la función library(): library(tidyverse) … y listo para usar. La razón por la cual activamos tidyverse es que en este momento nos vienen bien dos de sus funciones: mutate() para modificar valores, y ggplot() para hacer gráficos. Bien, llega la hora de los gráficos. Vamos a llamar a la función ggplot(), una auténtica navaja suiza para la visualización. Por ejemplo, veamos a cuanto asciende la tasa de mortalidad infantil en cada comuna durante 2016: ggplot(mortalidad) + geom_col(aes(x = factor(Comuna), y = Tasa2016)) Para realizar una visualización con ésta herramienta, siempre se comienza con la función ggplot(), que crea un eje de coordenadas sobre el cual se pueden agregar capas. El primer parámetro que recibe ggplot() es el dataframe que queremos usar para el gráfico; en nuestro caso, ggplot(mortalidad). Ejecutar sólo ggplot(mortalidad) nos devuelve un gráfico vacío; la gracia está en agregar una o más capas especificando cómo queremos mostrar los datos. Estas capas se agregan con un signo +. En nuestro ejemplo, geom_col() crea columnas cuya posición en el eje de las x depende de la variable “Comuna”, mientas que la altura (posición en el eje de las y) depende del valor de la variable “Tasa2016”. Existen muchas funciones de tipo “geom_XXX”, que agregan distintas clases de capas al gráfico: geom_point, geom_polygon, geom_text y muchos, muchos más que iremos viendo más adelante. Cada función “geom_” toma como parámetro un conjunto de definiciones “estéticas” que le indican una variable a graficar (“mortalidad” en nuestro caso), cómo (color, tamaño, etc) y dónde (posición x, posición y del eje). Estos parámetros van siempre dentro de una función auxiliar, aes(). En nuestro ejemplo, “geom_col(aes(x = factor(Comuna), y = Tasa2016))”. No se preocupen que iremos practicando el uso de ggplot, y su uso se volverá familiar. En cuanto al gráfico que hemos creado, podemos observar que entre las 15 comunas en la ciudad, la tasa de mortalidad tiene un rango que va de un poco menos de 2,5 a un poco más de 12,5 (esto es, muertes antes del año de vida por cada 10.000 nacimientos). Pero no se distingue aquello que queríamos comprender: la diferencia entre el norte y el sur de la ciudad. Necesitamos contexto geográfico. 2.2.1 Haciendo mapas Vamos a presentar un paquete más, el último para éste capítulo: sf. Quizás algunos tengan experiencia con sistemas de información geográfica (GIS por sus siglas en inglés), al estilo de QGIS o ArcGIS, que permiten crear, manipular y combinar archivos con datos espaciales para producir mapas que pueden ser simples o en extremo sofisticados. En R, el paquete sf brinda herramientas que permiten realizar tares similares. Nuestro objetivo es obtener un mapa de la ciudad de Buenos Aires con sus comunas. Primero, instalamos sf en caso de que aún no lo hayamos hecho. install.packages(&quot;sf&quot;) Vale la pena insistir: Sólo es necesario instalar los paquetes una vez. De aquí en más, cada vez que queramos echar mano a las funciones incluidas en sf, sólo necesitamos activarlo pues ya estará listo en nuestro sistema. Pedimos a R que active el paquete así: library(sf) Luego, cargamos un archivo georeferenciado con las comunas de la Ciudad Autónoma de Buenos Aires, disponible online en formato geojson, un estándar de representación de datos geográficos que es fácil de usar: comunas &lt;- st_read(&#39;https://bitsandbricks.github.io/data/CABA_comunas.geojson&#39;) ## Reading layer `CABA_comunas&#39; from data source ## `https://bitsandbricks.github.io/data/CABA_comunas.geojson&#39; ## using driver `GeoJSON&#39; ## Simple feature collection with 15 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -58.53152 ymin: -34.70529 xmax: -58.33514 ymax: -34.52754 ## Geodetic CRS: WGS 84 Al igual que cuando usamos read.csv() para leer un archivo .csv y cargarlo como un dataframe, el comando st_read() hace lo propio con archivos de información geográfica, conocidos en la jerga como “shapefiles”. El resultado también es un dataframe, por lo cual podemos practicar el uso de las funciones que ya aprendimos, como dim(), names() y head(). dim(comunas) ## [1] 15 5 names(comunas) ## [1] &quot;barrios&quot; &quot;perimetro&quot; &quot;area&quot; &quot;comunas&quot; &quot;geometry&quot; head(comunas) Podemos ver que el dataframe contiene 15 filas y 5 columnas. Una fila por comuna (es razonable!) y 5 columnas: “barrios”, “perímetro”, “area”, “comunas” y “geometry”. Nuestro vistazo mediante head() permite asumir que “barrios” informa los barrios que componen cada comuna, mientras que perímetro y área informan sobre las dimensiones del polígono cubierto por cada comuna. La columna “geometry” aparece en todos los dataframes de tipo espacial, y es la que contiene los datos con sus coordenadas geográficas. Y hablando de coordenadas, generar un mapa a partir de un dataframe espacial creado por sf es muy fácil con la ayuda de ggplot ggplot(comunas) + geom_sf() Si queremos agregar una leyenda al mapa que identifique cada comuna con su número, usamos: ggplot(comunas) + geom_sf(aes(fill = comunas)) Dentro de “aes()” usé el parámetro “fill” (relleno en inglés) para pedirle a ggplot que llene cada polígono con un color distinto de acuerdo al campo “comunas”. Aprovechando que tenemos un mapa, deberíamos clasificar las comunas entre las que pertenecen al norte y las que pertenecen al sur de la ciudad. No hay una línea divisoria oficial, pero la traza de la Avenida Rivadavia suele ser tomada como frontera: Rivadavia es la “divisoria simbólica del Norte y el Sur de la Ciudad, con sus diferencias de desarrollo” Por esas casualidades de la vida, tengo un archivo geográfico que contiene la línea que dibuja a avenida Rivadavia al atravesar la ciudad. Lo bajamos: rivadavia &lt;- st_read(&#39;https://bitsandbricks.github.io/data/avenida_rivadavia.geojson&#39;) ## Reading layer `avenida_rivadavia&#39; from data source ## `https://bitsandbricks.github.io/data/avenida_rivadavia.geojson&#39; ## using driver `GeoJSON&#39; ## Simple feature collection with 1 feature and 1 field ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -58.53014 ymin: -34.63946 xmax: -58.37017 ymax: -34.60711 ## Geodetic CRS: WGS 84 Y lo proyectamos sobre el mapa, como una capa adicional del gráfico de ggplot que definimos antes: ggplot(comunas) + geom_sf(aes(fill = comunas)) + geom_sf(data = rivadavia, color = &quot;red&quot;) La identificación por colores no hace fácil reconocer con rapidez que número corresponde a cada comuna; es un recurso que funciona mejor con menos categorías que nuestras 15. Podríamos arreglarlo, por ejemplo evitando la codificación por color, y dibujando una etiqueta con número dibujada sobre cada comuna. ¡Pero no en este momento! En aras de la sencillez, vamos a aguzar la vista y tomar nota de cuales comunas tienen gran parte de su territorio al sur de la Avenida Rivadavia. Según mi interpretación, son las comunas 1, 3, 4, 5, 7, 8 y 9. (Hay que admitir que la comuna 1 parece estar repartida en partes más o menos iguales, pero vamos a dejársela al sur en forma arbitraria para no complicar el ejercicio). 2.2.2 Agregando datos En este punto necesitamos una manera de “etiquetar” cada comuna con el punto cardinal que le toca “Norte” o “Sur”. La forma más rápida es crear una lista con los atributos, y agregarla a nuestro dataframe como una nueva columna. Podemos armar una sucesión de 15 “etiquetas” según el punto cardinal que le toca a cada comuna. El comando en R que “une” valores en conjunto se llama c() (viene de “combine”, “combinar”), y permite definir una lista de valores. Mejor dicho, un “vector” de valores; en el mundo de la programación, se usa la palabra vector cuando se combinan elementos del mismo tipo, y “lista” cuando se combina una variedad de clases: en el mismo conjunto números, textos, y otros tipos de objeto más complejos. Por ahora, no nos preocupemos por eso. nueva_columna &lt;- c(&quot;Sur&quot;, &quot;Norte&quot;, &quot;Sur&quot;, &quot;Sur&quot;, &quot;Sur&quot;, &quot;Norte&quot;, &quot;Sur&quot;, &quot;Sur&quot;, &quot;Sur&quot;, &quot;Norte&quot;, &quot;Norte&quot;, &quot;Norte&quot;, &quot;Norte&quot;, &quot;Norte&quot;, &quot;Norte&quot;) nueva_columna ## [1] &quot;Sur&quot; &quot;Norte&quot; &quot;Sur&quot; &quot;Sur&quot; &quot;Sur&quot; &quot;Norte&quot; &quot;Sur&quot; &quot;Sur&quot; &quot;Sur&quot; ## [10] &quot;Norte&quot; &quot;Norte&quot; &quot;Norte&quot; &quot;Norte&quot; &quot;Norte&quot; &quot;Norte&quot; Ya podemos agregar nuestra nueva columna usando una función que ya vimos, mutate(). En el dataframe, vamos a ponerle a nuestra nueva columna un nombre descriptivo, “ubicación” : comunas &lt;- mutate(comunas, ubicacion = nueva_columna) Verifiquemos el resultado: head(comunas) Y en el mapa: ggplot(comunas) + geom_sf(aes(fill = ubicacion)) + geom_sf(data = rivadavia, color = &quot;red&quot;) Todo en orden. Ahora hagamos lo mismo con el dataframe de mortalidad, aprovechando que lista las comunas en el mismo orden (del 1 al 15) y por lo tanto podemos “pegarle” el mismo vector de etiquetas con ubicación que ya preparamos. mortalidad &lt;- mutate(mortalidad, ubicación = nueva_columna) head(mortalidad) 2.3 El veredicto final Habrán notado que llegar hasta aquí tomó una buena cantidad de operaciones. En contraste, lo que estamos a punto de hacer -responder la pregunta inicial- va a ser mucho más breve. Esa vendría a ser la lección central de éste capítulo: la mayor parte del tiempo empleado en la labor de la ciencia de datos se insume en la poco glamorosa tarea de recopilar, limpiar y combinar los registros necesarios para el análisis. Como consuelo, podemos pensar en que el esfuerzo necesario para llegar a este punto nos ha dado un conocimiento de los datos (su estructura, sus limitaciones, su potencial) que no teníamos antes. Aprovechemos entonces nuestra data limpia y ordenada, para producir un mapa que señale con color el nivel de mortalidad. Armamos un ggplot con una capa que muestra las comunas, cuyo color interior (“fill”) depende del valor de la mortalidad. Le sumamos una capa con la traza de la Avenida Rivadavia, nuestra referencia de posición, y por último definimos la paleta de colores a usar en el fill, eligiendo una llamada “Spectral”, que va del azul al rojo y es muy usada cuando se quiere resaltar la divergencia de una variable. ggplot(comunas) + geom_sf(aes(fill = mortalidad$Tasa2016)) + geom_sf(data = rivadavia, color = &quot;red&quot;) + scale_fill_distiller(palette = &quot;Spectral&quot;) Para una comparación visual más precisa entre los valores de cada comuna, le pedimos a ggplot un gráfico de barras, con la capa geom_col(). En las variables estéticas, definimos que la posición de las barras en el eje de las x estará dada por el número de cada comuna, la altura de las barras (eje y) será dada por su tasa de mortalidad, y su color de relleno (fill) dependerá de su ubicación geográfica. ggplot(mortalidad) + geom_col(aes(x = Comuna, y = Tasa2016, fill = ubicación)) + labs(title = &quot;Mortalidad infantil en la Ciudad Autónoma de Buenos Aires&quot;, subtitle = &quot;Año 2016&quot;, y = &quot;tasa&quot;) 2.3.1 ¿Cuál es la diferencia en mortalidad infantil entre el sur y el norte de la Ciudad Autónoma de Buenos Aires? En base a lo que descubrimos, vamos a responder en forma sucinta: Según los registros del año 2016, la tasa de mortalidad infantil en cada uno de los barrios del sur es más alta que en cualquiera de los del norte Por supuesto, con esto no puede darse por cerrado el tema; hay muchas facetas que deberíamos analizar para comenzar a entender un fenómeno social de tal complejidad. Por ejemplo, ¿Cómo es la evolución en el tiempo de la brecha norte/sur - se mantiene igual, decrece, aumenta? ¿Qué otros factores están correlacionados con la disparidad, más allá del geográfico? En los siguientes capítulos practicaremos varias técnicas que nos permitirán profundizar nuestros análisis, en la nunca finalizada misión de entender un poco más. 2.4 Ejercicios I. Descarguemos un dataset en formato csv. Pueden recurrir al portal de datos abiertos de su ciudad, o a cualquier otro repositorio público, como el de bases de datos del Banco Mundial. Escribamos y ejecutemos el código R necesario para: leer el archivo y asignar su contenido a una variable mostrar los nombres de sus columnas mostrar sus dimensiones (cantidad de filas y columnas) mostrar un resumen del contenido de cada una de las columnas ¡Eso es todo por ahora! Al resolverlo, habremos aprendido dos tareas simples pero fundamentales: cómo acceder a nuestros propios datos, y como crear un reporte con nuestros resultados. Pistas: aquí hay un tutorial que explica paso a paso cómo abrir un archivo en .csv R, con la ayuda de RStudio las funciones read.csv(), names(), dim() y summary() serán nuestras amigas "],["poniendo-los-datos-en-forma.html", "3 Poniendo los datos en forma 3.1 Primeros pasos al examinar un conjunto de datos nuevo 3.2 Cruzando variables: la operación join 3.3 Transformando los datos 3.4 Ejercicios", " 3 Poniendo los datos en forma Cómo ya hemos mencionado, es normal que la mayor parte del tiempo dedicado a un proyecto de análisis se nos vaya en la limpieza y orden de los datos disponibles. Aún cuando nuestros datos provengan de fuentes oficiales (un gobierno nacional, el Banco Mundial, etc) en muy rara ocasión podremos usarlos para nuestros fines sin antes procesarlos. Y aún si los datos llegaran en perfectas condiciones, no tenemos forma de saberlo hasta haber realizado una exploración para verificarlo. Ésta inevitable etapa de preparación es llamada data wrangling en inglés, algo así como el proceso de “domar los datos”. El término hace referencia, en clave de humor, al esfuerzo que requiere la puesta en orden cuando los datos son cuantiosos, de muchas fuentes distintas, o en particular desprolijos. Para que la experiencia sea lo menos tediosa posible, y podamos pasar rápido al momento de extraer conocimiento, vamos a practicar algunas técnicas muy útiles de wrangling. 3.1 Primeros pasos al examinar un conjunto de datos nuevo Si no lo hicimos aún en la sesión en la que estamos trabajando, cargamos tidyverse. library(tidyverse) Vamos a practicar usando el dataset de delitos que publica el Ministerio de Justicia y Seguridad de la Ciudad Autónoma de Buenos Aires. Registra los homicidios, hurtos (sin violencia), lesiones y robos (con violencia) ocurridos en la ciudad durante el año 2020. Vamos a trabajar con una versión de los datos que ha sido simplificada para hacer más ameno el trabajo con ella. Quién quiera acceder a los datos en su esplendor de complejidad original, puede encontrarlos en el portal de datos abiertos de la ciudad: https://data.buenosaires.gob.ar/. Comenzamos por acceder al archivo con los registros para cargarlo en R como un dataframe. Esto requiere de una conexión a internet, pero no debería tomar mucho tiempo en ser descargado. delitos &lt;- read.csv(&quot;https://cdaj.netlify.app/data/delitos_barrios.csv&quot;) Lo primero que deberíamos hacer con un dataframe que no conocemos es usar la función str(), que nos indica su estructura (por structure en inglés): str(delitos) ## &#39;data.frame&#39;: 3188 obs. of 5 variables: ## $ periodo: chr &quot;01/2020&quot; &quot;01/2020&quot; &quot;01/2020&quot; &quot;01/2020&quot; ... ## $ tipo : chr &quot;Homicidio&quot; &quot;Homicidio&quot; &quot;Homicidio&quot; &quot;Homicidio&quot; ... ## $ subtipo: chr &quot;Doloso&quot; &quot;Doloso&quot; &quot;Doloso&quot; &quot;Doloso&quot; ... ## $ barrio : chr &quot;Barracas&quot; &quot;Nueva Pompeya&quot; &quot;Retiro&quot; &quot;Saavedra&quot; ... ## $ total : int 1 1 1 1 1 1 1 2 1 1 ... Para empezar, nos enteramos que el objeto que estamos analizando es un dataframe (“data.frame”). Eso ya lo sabíamos, pero como str() puede usarse con cualquier clase de objeto en R, en ocasiones resultará que estamos ante un vector, una lista u otra clase de criatura. A continuación aparecen las dimensiones del dataframe: 3188 observaciones (filas) con 5 variables (columnas). Los nombres de las columnas son periodo, tipo, subtipo, barrio y total. Con eso ya podemos inferir que cada observación en el dataframe contiene la cantidad total de delitos según tipo y subtipo (aunque no sepamos bien de que se tratan esas variables), en un período dado y en cada barrio. Con str() también obtenemos el tipo de datos representados por cada variable, y un ejemplo de los valores contenidos en las primeras filas. La variable total es de tipo “int”, es decir, números enteros o integers en inglés. El resto de las variables son de tipo “chr”; en R las variables de texto reciben el nombre de characters, caracteres. La siguiente función a utilizar cuando estamos conociendo el contenido de un set de datos es summary(), que nos dará un resumen en forma de estadísticas descriptivas para las variables numéricas (cuartiles y mediana) . summary(delitos) ## periodo tipo subtipo barrio ## Length:3188 Length:3188 Length:3188 Length:3188 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## total ## Min. : 1.00 ## 1st Qu.: 3.00 ## Median : 8.00 ## Mean : 20.93 ## 3rd Qu.: 22.00 ## Max. :393.00 Para las variables de texto summary() no tiene mucho que decir. Aquí sería bueno que nos mostrara algún dato, quizás los valores más frecuentes, ¿verdad?. Eso va a ocurrir pero sólo cuando esas columnas con texto contengan variables categóricas, que en R reciben el tipo “factor” en lugar de “chr”. Una variable es categórica cuando es razonable considerar que se elige entre un conjunto finito de variables posibles; por ejemplo, los barrios de Buenos Aires son un conjunto finito y predeterminado. ¿Pero cómo puede saber R si tipo o barrio son categorías? Pues podemos avisarle al cargar los datos con la función read.csv() que usamos al principio. Para que read.csv() interprete como variables categóricas a todas las columnas que contienen texto se usa el parámetro stringsAsFactors, así: misdatos &lt;- read.csv(\"archivo_con_mis_datos\", stringsAsFactors = TRUE). En general es buena idea evitar que los campos de texto se asuman como factores, pero en éste caso está bien: aquí todas las columnas de texto, en efecto, contienen variables categóricas. Veamos que pasa si lo intentamos. Volvemos a leer el mismo dataset, esta vez con stringsAsFactors = TRUE delitos &lt;- read.csv(&quot;https://cdaj.netlify.app/data/delitos_barrios.csv&quot;, stringsAsFactors = TRUE) .. y pedimos su resumen: summary(delitos) ## periodo tipo subtipo ## 01/2020: 284 Homicidio : 163 :1166 ## 02/2020: 281 Hurto (sin violencia):1087 Siniestro Vial : 644 ## 03/2020: 280 Lesiones : 575 Con uso de moto: 517 ## 12/2020: 280 Robo (con violencia) :1363 Hurto Automotor: 504 ## 08/2020: 276 Robo Automotor : 263 ## 11/2020: 274 Doloso : 81 ## (Other):1513 (Other) : 13 ## barrio total ## Balvanera : 81 Min. : 1.00 ## Barracas : 81 1st Qu.: 3.00 ## Flores : 80 Median : 8.00 ## Villa Lugano : 80 Mean : 20.93 ## Mataderos : 76 3rd Qu.: 22.00 ## Villa Soldati: 75 Max. :393.00 ## (Other) :2715 ¡Esta vez obtuvimos un ranking con las categorías más frecuentes! Las categorías posibles para un factor son llamadas “niveles” (levels). Para ver todos los niveles del factor barrio, es decir todos los barrios representados en la columna con ese nombre de variable, podemos usar la función levels() levels(delitos$barrio) ## [1] &quot;&quot; &quot;Agronomía&quot; &quot;Almagro&quot; ## [4] &quot;Balvanera&quot; &quot;Barracas&quot; &quot;Belgrano&quot; ## [7] &quot;Boca&quot; &quot;Boedo&quot; &quot;Caballito&quot; ## [10] &quot;Chacarita&quot; &quot;Coghlan&quot; &quot;Colegiales&quot; ## [13] &quot;Constitución&quot; &quot;Flores&quot; &quot;Floresta&quot; ## [16] &quot;La Boca&quot; &quot;Liniers&quot; &quot;Mataderos&quot; ## [19] &quot;Monserrat&quot; &quot;Monte Castro&quot; &quot;Nueva Pompeya&quot; ## [22] &quot;Nuñez&quot; &quot;Palermo&quot; &quot;Parque Avellaneda&quot; ## [25] &quot;Parque Chacabuco&quot; &quot;Parque Chas&quot; &quot;Parque Patricios&quot; ## [28] &quot;Paternal&quot; &quot;Puerto Madero&quot; &quot;Recoleta&quot; ## [31] &quot;Retiro&quot; &quot;Saavedra&quot; &quot;San Cristóbal&quot; ## [34] &quot;San Nicolás&quot; &quot;San Telmo&quot; &quot;Vélez Sársfield&quot; ## [37] &quot;Versalles&quot; &quot;Villa Crespo&quot; &quot;Villa Del Parque&quot; ## [40] &quot;Villa Devoto&quot; &quot;Villa Gral. Mitre&quot; &quot;Villa Lugano&quot; ## [43] &quot;Villa Luro&quot; &quot;Villa Ortuzar&quot; &quot;Villa Pueyrredón&quot; ## [46] &quot;Villa Real&quot; &quot;Villa Riachuelo&quot; &quot;Villa Santa Rita&quot; ## [49] &quot;Villa Soldati&quot; &quot;Villa Urquiza&quot; Para acceder en forma rápida al contenido de la columna barrio, hemos utilizado por primera vez un truco muy práctico. Para obtener el contenido de cualquier columna en particular, basta con el nombre del dataframe seguido del símbolo $ y el nombre de la columna a extraer: delitos$tipo, o delitos$total, etc. 3.2 Cruzando variables: la operación join Al realizar un análisis “en la vida real”, es decir, usando datos salvajes en lugar de los prolijos datasets de práctica, es muy habitual encontrar que nos falta una variable que necesitamos. Si tenemos suerte, la información que necesitamos también está disponible en forma de tabla, con algún campo en común, y podemos llevar el cabo un cruce de datos para traérnosla. Para expresarlo con un ejemplo concreto: hemos visto que los registros de delitos incluyen una columna con el barrio, que es la única variable relacionada con la geografía. Si nuestra unidad de análisis fuera la comuna3 en lugar del barrio, necesitaríamos agregar la columna correspondiente. En este caso, estamos de suerte porque una tabla con los barrios de la Ciudad de Buenos Aires y la comuna a la que pertenecen es fácil de conseguir. Con esa tabla en nuestro poder, ya tenemos las piezas necesarias para el cruce de datos. En cada registro en el dataframe de delitos tenemos un barrio; podemos buscarlo en la tabla de barrios y comunas, tomar nota de la comuna asociada, y copiarla en nuestro dataset original. Por supuesto, hacerlo a mano para cada uno de las miles de filas en nuestro dataframe tardaría una eternidad, amén de que quizás podríamos hartarnos de la tarea antes de terminar. ¡Nada de eso! Vamos a resolverlo en meros instantes escribiendo unas pocas líneas de código. Antes de continuar hagamos una pausa para conmiserar a los investigadores de eras pasadas, antes de la popularización de la computadora personal, que realizaban tareas de esta escala con lápiz, papel y paciencia. Existe una gran variedad de funciones que permiten combinar tablas relacionadas entre sí por una o varias variables en común. Para nuestro propósito, alcanza con conocer una: left_join(). La función toma como parámetros dos dataframes (que son tablas al fin y al cabo) busca las variables que tengan el mismo nombre y usándolas como referencia completa la primera de ellas, la de la izquierda, con los datos nuevos que aporta la segunda. left_join devuelve un dataframe nuevo con los datos combinados. Manos a la obra. Descargamos el dataframe con barrios y comunas: barrios_comunas &lt;- read.csv(&quot;https://cdaj.netlify.app/data/barrios_comunas.csv&quot;) Si recibimos un mensaje de error al estilo de [...] Error in left_join(delitos, barrios_comunas) : could not find function \"left_join\" es muy probable que nos hayamos olvidado de activar las funciones de Tidyverse. En ese caso, sólo necesitamos ejecutar library(tidyverse) e intentar de nuevo. Echamos un vistazo, comprobando que existe “barrios”, una columna en común que lo relaciona con el dataframe de delitos, barrios_comunas y lo unimos (de allí el término “join”, unir en inglés) a nuestra data: delitos &lt;- left_join(delitos, barrios_comunas) Admiremos nuestra obra: head(delitos) Es así de fácil. Bueno, no tanto… este fue un caso sencillo, pero hay todo tipo de datos y cruces allí afuera, y a veces se necesitan operaciones más complejas. Por eso hay toda una familia de funciones de join - right_join(), inner_join(), full_join, anti_join(), y alguna más. Pero podemos dejarlas en paz; para nuestras necesidades, con left_join() podemos arreglarnos muy bien. Satisfechos con la mejora, si queremos guardar el dataframe “mejorado” para usarlo en otra ocasión, podemos hacerlo con write.csv(), que lo convierte en un archivo de texto que queda en nuestra PC. write.csv(delitos, &quot;delitos.csv&quot;, row.names = FALSE) Podemos seguir siempre ese formato para guardar nuestros datos. El primer parámetro es el dataframe que vamos a guardar, el segundo -siempre entre comillas- es el nombre de archivo, y la opción final, row.names = FALSE sirve para evitar que R le agregue una columna al principio con números consecutivos (1, 2, 3, y así), cosa que quizás fue útil alguna vez pero en general no necesitamos. Para volver a leer los datos en otra ocasión, usamos read.csv() tal como ya hemos hecho. delitos &lt;- read.csv(&quot;delitos.csv&quot;) Y si queremos saber exactamente dónde ha guardado R nuestros datos, por ejemplo para abrirlos con otro programa, usamos la función getwd (por get working directory ) getwd() El resultado será la dirección (la ubicación de la la carpeta), donde estamos trabajando y hemos guardado los datos; por ejemplo /home/antonio/Practicando R/. 3.3 Transformando los datos Habiendo revisado el contenido de un dataframe (y agregado alguna variable si hiciera falta), comenzamos a hacernos idea de los ajustes que necesita para que los datos tomen el formato que necesitamos. Estos ajustes pueden ser correcciones (por ejemplo, de errores de tipeo cuando se cargaron los datos), la creación de nuevas variables derivadas de las existentes, o un reordenamiento de los datos para simplificar nuestro trabajo. Para hacer todo esto, y mucho más, vamos a aprender funciones que representan cinco verbos básicos para la transformación de datos: select(): seleccionar -elegir- columnas por su nombre filter(): filtrar, es decir quedarse sólo con las filas que cumplan cierta condición arrange(): ordenar las filas de acuerdo a su contenido o algún otro índice mutate(): mutar -cambiar- un dataframe, modificando el contenido de sus columnas o creando columnas (es decir, variables) nuevas summarise(): producir sumarios -un valor extraído de muchos, por ejemplo el promedio- con el contenido de las columnas Estas funciones tienen una sintaxis, una forma de escribirse, uniforme. El primer argumento que toman siempre es un dataframe; los siguientes indican qué hacer con los datos. El resultado siempre es un nuevo dataframe. Las funciones son parte de dplyr, uno de los componentes de la familia de paquetes Tidyverse. Ya tenemos disponible todo lo necesario, activado cuando invocamos library(tidyverse) al comienzo. Manos a la obra. 3.3.1 Seleccionar columnas con select() Muchas veces tendremos que lidiar con datasets con decenas de variables. Alguna que otra vez, con centenas. En esos casos el primer problema es librarnos de semejante cantidad de columnas, reteniendo sólo aquellas en las que estamos interesados. Para un dataset como el de reclamos de los ciudadanos, que tiene pocas columnas, select() no es tan importante. Aún así, podemos usar select() con fines demostrativos. Sabemos que el dataset tiene 6 columnas: names(delitos) ## [1] &quot;periodo&quot; &quot;tipo&quot; &quot;subtipo&quot; &quot;barrio&quot; &quot;total&quot; &quot;comuna&quot; Si quisiéramos sólo las que contienen el período y el total, las seleccionamos por nombre, a continuación del nombre del dataframe: seleccion &lt;- select(delitos, periodo, total) head(seleccion) También podemos seleccionar por contigüidad, por ejemplo “todas las columnas que van de tipo a barrio”: seleccion &lt;- select(delitos, tipo:barrio) head(seleccion) Y podemos seleccionar por omisión. Si nos interesara todo el contenido del dataset menos la variable subtipo, usaríamos seleccion &lt;- select(delitos, -subtipo) head(seleccion) Al igual que con las selección por inclusión, podemos seleccionar por omisión de un rango de columnas contiguas (escritas entre paréntesis), o de varias columnas nombradas: seleccion &lt;- select(delitos, -(subtipo:total)) head(seleccion) seleccion &lt;- select(delitos, -tipo, -barrio) head(seleccion) 3.3.2 Filtrar filas con filter() Una de las tareas más frecuentes en el análisis de datos es la de identificar observaciones que cumplen con determinada condición. filter() permite extraer subconjuntos del total en base a sus variables. Por ejemplo, para seleccionar registros que correspondan a Retiro, ocurridos en el primer mes de 2020 (período “01/2020”): seleccion &lt;- filter(delitos, barrio == &quot;Retiro&quot;, periodo == &quot;01/2020&quot;) head(seleccion) 3.3.2.1 Comparaciones Aquí hemos usado un recurso nuevo, la comparación. R provee una serie de símbolos que permite comparar valores entre sí: == igual a != no igual (distinto) a &gt; mayor a &gt;= mayor o igual a &lt; menor a &lt;= menor o igual a Atención especial merece el símbolo que compara igualdad, ==. Un error muy común es escribir barrio = \"Retiro\", (un sólo símbolo =) que le indica a R que guarde el valor “Retiro” dentro de la variable barrio, en lugar de verificar si son iguales. Para ésto último, lo correcto es barrio == \"Retiro\", tal como lo usamos en el ejemplo para filter(). También hay que tener en cuenta el uso de comillas. Para que R no se confunda, cuando queramos usar valores de texto (de tipo character) los rodeamos con comillas para que quede claro que no nos referimos a una variable con ese nombre, si la hubiera, sino en forma literal a esa palabra o secuencia de texto. En el caso de los números, no hace falta el uso de comillas, ya que en R ningún nombre de variable puede comenzar con o estar compuesta sólo por números. Filtrando los registros de períodos para los cuales se registran más de 100 incidentes: seleccion &lt;- filter(delitos, total &gt; 100) head(seleccion) 3.3.2.2 Operadores lógicos Cuando le pasamos múltiples condiciones a filter(), la función devuelve las filas que cumplen con todas. Por ejemplo, con seleccion &lt;- filter(delitos, periodo == &quot;08/2020&quot;, tipo == &quot;Homicidio&quot;) head(seleccion) obtenemos todos los registros cuyo rubro es “Homicidio”, y cuyo período es 08/2020, agosto de 2020. Siguiendo el mismo formato, si intentamos seleccion &lt;- filter(delitos, barrio == &quot;Retiro&quot;, barrio == &quot;Palermo&quot;) head(seleccion) obtenemos un conjunto vacío. ¿Por qué? Es debido a que ninguna observación cumple con todas las condiciones; en ningún registro el barrio es Retiro y también es Palermo. ¡Suena razonable!. Para obtener registros ocurridos en Retiro ó en Palermo, usamos el operador lógico | que significa… “ó”. seleccion &lt;- filter(delitos, barrio == &quot;Retiro&quot; | barrio == &quot;Palermo&quot;) head(seleccion) Se trata de la lógica de conjuntos, o lógica booleana, que con un poco de suerte recordamos de nuestra época de escolares. Los símbolos importantes son &amp;, |, y !: “y”, “ó”, y la negación que invierte preposiciones: a &amp; b a y b a | b a ó b a &amp; !b a, y no b !a &amp; b no a, y b !(a &amp; b) no (a y b) Hemos visto ejemplos de a &amp; b (periodo == \"08/2020\", tipo == \"Homicidio\", que filter() toma como un &amp;) y de a | b (cuando optamos por barrio == \"Retiro\" | barrio == \"Palermo\") Un ejemplo de a &amp; !b : pedimos filas en las que el tipo sea “Robo (con violencia)”, y además el subtipo no sea “Robo Automotor”: filter(delitos, tipo == &quot;Robo (con violencia)&quot; &amp; !(subtipo == &quot;Robo Automotor&quot;)) Y como ejemplo de !(a &amp; b), todas las filas excepto las de tipo “Homicidio” con subtipo “Siniestro Vial”: seleccion &lt;- filter(delitos, !(tipo == &quot;Homicidio&quot; &amp; subtipo == &quot;Siniestro Vial&quot;)) head(seleccion) 3.3.3 Ordenar filas con arrange() La función arrange() cambia el orden en el que aparecen las filas de un dataframe. Como primer parámetro toma un dataframe, al igual que el resto de los verbos de transformación que estamos aprendiendo. A continuación, espera un set de columnas para definir el orden. Por ejemplo, para ordenar por total de registros: ordenado &lt;- arrange(delitos, total) head(ordenado) Si agregamos más columnas, se usan en orden para “desempatar”. Por ejemplo, si queremos que las filas con el mismo valor en total aparezcan en el orden alfabético del barrio que les corresponde, sólo necesitamos agregar esa columna: ordenado &lt;- arrange(delitos, total, barrio) head(ordenado) Si no se aclara lo contrario, el orden siempre es ascendente (de menor a mayor). Si quisiéramos orden de mayor a menor, usamos desc(): ordenado &lt;- arrange(delitos, desc(total)) head(ordenado) 3.3.3.1 Valores faltantes En el último ejemplo, aparecen varias filas cuyo valor para la columna BARRIO es NA. R representa los valores ausentes, desconocidos, con NA (“no disponible”, del inglés Not Available). Hay que tener cuidado con los valores NA, porque la mayoría de las comparaciones y operaciones lógicas que los involucran resultan indefinidas. En la práctica: ¿Es 10 mayor a un valor desconocido? 10 &gt; NA ## [1] NA R no sabe. (Nadie lo sabe, para ser justos) ¿A cuanto asciende la suma de 10 más un valor desconocido? NA + 10 ## [1] NA Y en particular… ¿es un valor desconocido igual a otro valor desconocido? NA == NA ## [1] NA Por supuesto, la respuesta es desconocida también. La insistencia de R en no definir operaciones que involucran NA’s podría parecer irritante a primera vista, pero en realidad nos hace un favor. Al evitar extraer conclusiones cuando trata con datos faltantes, nos evita caer en errores garrafales en los casos en que analizamos y comparamos datos incompletos. Además, podemos preguntar a R si un valor es desconocido, y allí si contesta con seguridad. La función requerida es is.na(). desconocido &lt;- NA is.na(desconocido) ## [1] TRUE Algo más a tener en cuenta con los valores desconocidos es cómo son interpretados cuando usamos funciones de transformación de datos. Por ejemplo, filter() ignora las filas que contienen NA’s en la variable que usa para filtrar. arrange() muestra las filas con NA’s en el campo por el que ordena, pero todas al final. 3.3.4 Agregar nuevas variables con mutate() Recurrimos a la función mutate() cuando queremos agregarle columnas adicionales a nuestro dataframe, en general en base a los valores de las columnas ya existentes. Vamos a ilustrarlo con un ejemplo sencillo. Imaginemos que tenemos el siguiente dataset: circulos &lt;- data.frame(nombre = c(&quot;Círculo 1&quot;, &quot;Círculo 2&quot;, &quot;Círculo 3&quot;), tamaño = c(&quot;Pequeño&quot;, &quot;Mediano&quot;, &quot;Grande&quot;), radio = c(1, 3, 5)) circulos Podemos agregar una columna con el área de cada círculo con mutate(): mutate(circulos, area = 3.1416 * radio^2) Usando mutate(), definimos la columna “area”, indicando que su contenido será el valor de la columna “radio” en cada registro puesto en la fórmula del área de un círculo. Los operadores aritméticos (+, -, *, /, ^) son con frecuencia útiles para usar en conjunto con mutate(). Volvamos ahora a nuestro dataframe con datos de delitos. Supongamos que nos interesa agregar columnas con el mes y el año de cada registro. La columna período, con valores del tipo “01/2020”, contiene la información necesaria para obtener estas dos nuevas variables. Para separar la parte del año de la parte del mes, la función substr(), que extrae porciones de una variable de texto, nos va a dar una mano. La usamos así: el primer parámetro es una secuencia de caracteres, y los dos siguientes indican donde queremos que empiece y termine la porción a extraer. delitos &lt;- mutate(delitos, mes = substr(periodo, 1, 2), año = substr(periodo, 4, 7)) head(delitos) 3.3.5 Extraer resúmenes con summarise() Llegamos al último de los verbos fundamentales para transformar datos. summarise() (por “resumir” en inglés) toma un dataframe completo y lo resume un una sola fila, de acuerdo a la operación que indiquemos. R ofrece una función que calcula promedios, mean(), que podríamos usar para obtener el promedio de la columna “total”: summarise(delitos, promedio = mean(total)) Bien, la cantidad promedio de delitos registrados en un mismo barrio y en un mismo mes, en 2020, es esa. Pero en general necesitamos un poco más de detalle sobre los datos que estamos explorando. Es que por si sola, summarise() no es de mucha ayuda. La gracia está en combinarla con group_by(), que cambia la unidad de análisis del dataframe completo a grupos individuales. Usar summarise() sobre un dataframe al que antes agrupamos con group_by resulta en resúmenes “por grupo”. agrupado &lt;- group_by(delitos, año) summarise(delitos, promedio_totales = mean(total)) Podemos agrupar por múltiples columnas, generando más subgrupos; por ejemplo, promedios por año y mes… agrupado &lt;- group_by(delitos, año, mes) sumario &lt;- summarise(agrupado, promedio = mean(total)) head(sumario) … o por año, mes y barrio: agrupado &lt;- group_by(delitos, año, mes, barrio) sumario &lt;- summarise(agrupado, promedio = mean(total)) head(sumario) Con summarise() podemos usar cualquier función que tome una lista de valores y devuelva un sólo resultado. Para empezar, algunas de las que más podrían ayudarnos son: mean(): Obtiene el promedio de los valores sum(): Obtiene la suma min(): Obtiene el valor más bajo max(): Obtiene el valor más alto R es cauteloso cuando se encuentra con nuestros amigos los datos faltantes, o NA’s: si le pedimos que calcule el promedio de una columna que contiene datos faltantes, indica que el resultado es desconocido. ¡Dice que el resultado es NA!. Lo mismo ocurre con otras funciones que también extraen un valor único, de resumen, a partir de un conjunto de datos (como sum(), min(), etc). Esto tiene sentido si lo pensamos así: el valor promedio de un conjunto de mediciones que son “5”, “8”, y “desconocido” es efectivamente desconocido, porque nos falta un dato para poder saberlo con certeza. Por otra parte, es común que nos alcance con saber cual es el resultado para los valores conocidos, y aceptamos el riesgo de ignorar los que no tenemos. Para esos casos existe el parámetro na.rm, que descarta los faltantes antes de hacer el cálculo. Lo usamos así: mean(total, na.rm = TRUE) 3.3.6 ¡BONUS! El operador “pipe”: %&gt;% Antes de terminar, vamos a presentar una herramienta más: el operador pipe (pronúnciese “paip”, es el término en inglés que significa “tubo”). El pipe es un operador: un símbolo que relaciona dos entidades. Dicho en forma más simple, el pipe de R, cuyo símbolo es %&gt;% está en familia con otros operadores más convencionales, como +, - o /. Y al igual que los otros operadores, entrega un resultado en base a los operandos que recibe. Ahora bien… ¿Para qué sirve? En resumidas cuentas, hace que el código necesario para realizar una serie de operaciones de transformación de datos sea mucho más simple de escribir y de interpretar. Por ejemplo, si quisiéramos obtener el top 5 de los barrios que más reclamos y denuncias de los ciudadanos han registrado durante marzo, la forma de lograrlo en base a lo que ya sabemos sería así: Filtramos los datos para aislar los registros de marzo; agrupamos por barrio; hacemos un resumen, creando una variable resumen que contiene la suma de los registros para cada barrio; los ordenamos en forma descendiente, mostramos sólo los primeros 5 (esto se puede hacer con la función head(), aclarando cuantas filas queremos ver) En código: solo_marzo &lt;- filter(delitos, mes == &quot;03&quot;) solo_marzo_agrupado_barrio &lt;- group_by(solo_marzo, barrio) total_por_barrio_marzo &lt;- summarise(solo_marzo_agrupado_barrio, total = sum(total)) total_por_barrio_marzo_ordenado &lt;- arrange(total_por_barrio_marzo, desc(total)) head(total_por_barrio_marzo_ordenado, 5) ¡Funciona! Pero… el problema es que hemos generado un puñado de variables (“solo_marzo”, “solo_marzo_agrupado_barrio”, etc) que, es probable, no volveremos a usar. Además de ser inútiles una vez obtenido el resultado buscado, estas variables intermedias requieren que las nombremos. Decidir el nombre de estas variables que no nos importan toma tiempo (sobre todo cuando producimos muchas), y nos distrae de lo importante, que es el análisis. El pipe, %&gt;%, permite encadenar operaciones, conectando el resultado de una como el dato de entrada de la siguiente. La misma secuencia que realizamos antes puede resolverse con pipes, quedando así: delitos %&gt;% filter(mes == &quot;03&quot;) %&gt;% group_by(barrio) %&gt;% summarise(total = sum(total)) %&gt;% arrange(desc(total)) %&gt;% head(5) Una manera de pronunciar %&gt;% cuando leemos código es “y luego…”. Algo así como “tomamos el dataframe”delitos\" y luego filtramos los registros del mes “03”, y luego agrupamos por barrio, y luego calculamos el total de registros para cada grupo, y luego los ordenamos en forma descendente por total, y luego vemos los cinco primeros\". El uso de pipes permite concentrarse en las operaciones de transformación, y no en lo que está siendo transformado en cada paso. Esto hace al código mucho más sencillo de leer e interpretar. En el ejemplo con pipe, sólo tuvimos que nombrar un dataframe con el cual trabajar un única vez, al principio. Detrás de escena, x %&gt;% f(y) se transforma en f(x, y). Por eso, filter(delitos, mes == &quot;03&quot;) es equivalente a delitos %&gt;% filter(mes == &quot;03&quot;) Trabajar con pipes es una de las ventajas que hacen de R un lenguaje muy expresivo y cómodo para manipular datos, y a partir de aquí lo usaremos de forma habitual. Con esto cerramos la sección de transformación de datos. Las técnicas para examinar un dataframe, como sumamry() nos permiten entender de forma rápida con que clase de variables vamos a trabajar. Los cinco verbos de manipulación que aprendimos, usados en conjunto, brindan una enorme capacidad para adaptar el formato de los datos a nuestras necesidades. Y el operador pipe nos ayuda a escribir nuestro código de forma sucinta y fácil de interpretar. A medida que vayamos progresando en nuestra familiaridad con las funciones -y agregando técnicas nuevas- vamos a ser capaces de procesar grandes cantidades de datos con soltura. Y obtener en pocos minutos lo que de otra forma, sin herramientas computacionales, tardaría días o sería inviable por lo tedioso. 3.4 Ejercicios I. Tomemos otro dataset, como el que descargamos durante los ejercicios del capítulo anterior, e intentemos: usar select() para seleccionar ciertas columnas y/o cambiar su orden usar filter() para filtrar las filas, descartando las que no cumplan cierta condición o condiciones a elección usar arrange() para ordenar el dataframe según alguna de sus variables usar mutate() para crear una o más columnas nuevas usar en combinación group_by() y summarise() para generar un resumen del dataset, que contenga al menos 3 variables (se pueden usar medias, máximos, conteos, etc) Si antes hicimos todos los pasos por separado, ahora intentemos hacer todo el procedimiento en una sola cadena de instrucciones usando el operador pipe: %&gt;% La Ciudad de Buenos Aires se encuentra organizada en 15 Comunas. Son unidades descentralizadas de gestión política y administrativa que, en general, abarcan varios barrios.↩︎ "],["visualización.html", "4 Visualización 4.1 Una buena visualización para empezar: el scatterplot 4.2 Ajustando color y tamaño 4.3 Facetado 4.4 Gráficos de barras 4.5 Histogramas 4.6 Preparando una visualización para compartir 4.7 Otras visualizaciones 4.8 Ejercicios", " 4 Visualización La visualización de información es una de las técnica más poderosas, y a la vez más accesibles, de las que disponemos como analistas de datos. La visualización es el proceso de hacer visibles los contrastes, ritmos y eventos que los datos expresan, que no podemos percibir cuando vienen en forma de áridas listas de números y categorías. Vamos a aprender a realizar las visualizaciones más usadas, y las opciones de ajuste con las que podemos lograr que luzcan tal como queremos. 4.1 Una buena visualización para empezar: el scatterplot Los gráficos de dispersión, o scatterplots, son quizás el tipo de visualización más conocido. Consisten en puntos proyectados en un eje de coordenadas, donde cada punto representa una observación. Son útiles para mostrar la correlación entre dos variables numéricas. Por ejemplo, podríamos asumir que existirá una correlación positiva entre la cantidad de habitantes de una comuna y la cantidad anual de delitos. Es decir que, cuantas más personas vivan en una comuna, es de esperarse que sea mayor la cantidad de robos, hurtos, siniestros viales y homicidios que ocurren allí. Activamos el paquete tidyverse, si aún no lo habíamos hecho. library(tidyverse) Y si no lo tenemos ya cargado, leemos de nuevo el dataset con los registros de delitos (esta versión incluye la columna “comuna”). delitos &lt;- read.csv(&quot;https://cdaj.netlify.app/data/delitos_barrios_comunas.csv&quot;) Usando los verbos de transformación que aprendimos, es fácil obtener un dataframe resumen con indicadores a nivel comuna. Vamos a analizar la cantidad de homicidios. homicidios_por_comuna &lt;- delitos %&gt;% filter(tipo == &quot;Homicidio&quot;) %&gt;% group_by(comuna) %&gt;% summarise(homicidios = sum(total)) homicidios_por_comuna Lo que nos falta ahora es la cantidad de habitantes en cada comuna. No problem. El dato es fácil de conseguir, otra vez cortesía de la Dirección General de Estadística y Censos de la Ciudad de Buenos Aires. Traemos la proyección al año 2020 de la cantidad de habitantes por comuna. poblacion &lt;- read.csv(&quot;https://cdaj.netlify.app/data/comunas_poblacion_2020.csv&quot;) poblacion Por suerte, ya sabemos como combinar tablas usando left_join() homicidios_por_comuna &lt;- homicidios_por_comuna %&gt;% left_join(poblacion) homicidios_por_comuna !Preparativos terminados! Hagamos por fin nuestro scatterplot. Tal como en el capítulo de introducción a R, continuaremos usando ggplot() para visualizar: ggplot(homicidios_por_comuna) ¿Un gráfico vacío? Recordemos que ggplot funciona por capas. Primero uno declara el dataframe que va a usar, y luego agrega una o más capas con representaciones de la información. La forma de agregar una capa con un scatterplot, en la práctica dibujar puntos, es con geom_point: ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios)) Lo que hicimos fue pedirle a ggplot que dibuje un punto por cada fila (representando a cada comuna), con la posición en el eje de las x según su población, y en el eje de las y según la cantidad de contactos registrados. Estas referencias estéticas (aesthetics en inglés) son las que van dentro de la función aes() en geom_point(aes(x = poblacion, y = homicidios)) En el extremo superior derecho hay una comuna que sobresale. Podemos identificarla, pidiendo a ggplot que agregue una variable más a la visualización -la comuna. Siendo un gráfico en dos dimensiones, ya no podemos usar la posición para representar un valor; tanto la posición horizontal como la vertical están siendo usadas por población y total. Nuestras opciones son codificar la comuna por color, forma o tamaño del punto. A pesar de que son identificadas con números, las comunas son una variable categórica: no tiene sentido decir que la comuna 1 es “menor” que la comuna 7. Par las variables categóricas, el color suele ser una buena opción de codificación. Lo hacemos agregando un parámetro color dentro de aes(). Tal como hicimos en el capítulo 2, usamos factor(comuna) en lugar de comuna a secas para indicarle a R que queremos que trate a la variable como categórica: ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios, color = factor(comuna))) En ese caso, no es tan fácil discernir cuál es cuál, pero mirando con cuidado descubrimos que la comuna 1 es la que tiene mayor población y donde se reportaó la mayor cantidad de homicidios. Lo que nos pasa aquí es que tenemos demasiadas categorías, con lo cual cada una tiene su propio color pero el rango cromático no alcanza para darle a cada una un tono bien distinto al de las demás. Si necesitamos generar un gráfico que no deje lugar a dudas, lo resolvemos usando un método alternativo para el scatterplot. En lugar de dibujar puntos, podemos poner etiquetas con el nombre de cada comuna. En lugar de ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios, color = factor(comuna))) usamos ggplot(homicidios_por_comuna) + geom_label(aes(x = poblacion, y = homicidios, label = factor(comuna))) Volvamos a nuestros puntos para practicar dos codificaciones estéticas que no hemos probado, color y tamaño. Para dejar aún más clara la diferencia de homicidios entre comunas, podríamos usar el tamaño (size) de cada punto para representar esa variable, además de su altura en el gráfico. ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios, size = homicidios)) Y para distinguir cuál es cuál, podemos pedirle a ggplot que cambie la forma (shape) de cada punto según la comuna a la que corresponde. ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios, shape = factor(comuna))) ¡Hey, sólo aparecen seis de las comunas! ggplot() usa cómo máximo 6 formas distintas, debido a que una cantidad mayor sería de veras muy difícil de discernir para nuestros pobres ojos. Moraleja: la estética shape sirve sólo cuando manejamos pocas categorías. De todas formas podría opinarse que es el método de codificación que menos gracia tiene, así que no es grave que su utilidad sea limitada. 4.2 Ajustando color y tamaño Hemos visto que especificando atributos estéticos y las variables que representan dentro de aes() podemos ajustas posición, tamaño, color y hasta la forma de los puntos de acuerdo a sus valores. Pero, ¿qué pasa si queremos usar un tamaño o un color arbitrario para nuestros puntos? Es decir, si no nos gusta el color negro y queremos que sean todos azules, o si nos parece que se ven pequeños y queremos que sean todos un poco más grandes. Fácil: definimos el color o size que queremos por fuera de las función aes(), y será aplicado a todos los puntos. ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios), color = &quot;blue&quot;) Obsérvese que color = \"blue\" está escrito por fuera de los paréntesis de aes(). De paso, hicimos uso de una característica muy práctica de R: reconoce un montón de colores por su nombre, siempre que los escribamos entre comillas. Si le decimos color = \"blue\", color = \"red\", color = \"yellow\", etc., sabe de que hablamos. Una lista de todos los colores que R reconoce, ideal como referencia, se puede encontrar en http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf ; ¡son más de 600!. Tras un vistazo a la lista, me decido por “darkolivegreen4”: ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios), color = &quot;darkolivegreen4&quot;) Queda claro que podemos asignar el color que queramos. En cuanto al tamaño, la fórmula es la misma: ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios), size = 5) El valor de size se da en píxeles. Es una medida difícil de estimar antes de ver el resultado, pero es cuestión de probar algunos valores distintos hasta encontrar el que nos va bien. Por supuesto, podemos ajustar varios, o todos, los atributos a la vez ggplot(homicidios_por_comuna) + geom_point(aes(x = poblacion, y = homicidios), size = 9, color = &quot;chocolate3&quot;, shape = 0) 4.3 Facetado Ya sabemos como representar variables usando atributos estéticos. Con esa técnica podemos mostrar con claridad dos o tres variables en un plano bidimensional (nuestro gráfico). Pero cuando si queremos agregar más atributos para codificar variables adicionales, la visualización pierde legibilidad de inmediato. Por suerte existe otra técnica, que podemos usar en combinación con la estética, para agregar aún más variables: el facetado. Las facetas son múltiples gráficos contiguos, con cada uno mostrando un subconjunto de los datos. Son útiles sobre todo para variables categóricas. Practiquemos con un ejemplo. Sabemos que en la comuna 1 se registra una cantidad de homicidios mayor que en las demás. ¿La diferencia será igual para todas las categorías de homicidio, o existe alguna en particular que es la que inclina la balanza? En nuestro dataframe original, para cada incidente tenemos las columnas “tipo” y “subtipo”. Para revisar rápido cuales son las combinaciones posibles de éstas categorías podemos armar una tabla de contingencia, un conteo de las combinaciones de categorías según aparecen en nuestros datos. Presentemos entonces a la función count(), que nos puede dar una mano. delitos %&gt;% count(tipo, subtipo) Para el tipo “Homicidio” encontramos los subtipos “Doloso”, “Femicidio”, “Femicidio Intrafamiliar”, “Siniestro Vial”, y “Travesticidio/Transfemicidio”. Podemos usar esas subcategorías para mostrar como componen el total de homicidios de cada comuna. Agrupamos entonces nuestra data de homicidios por comuna y también por subtipo, sin olvidar agregar luego los datos de población homicidios_por_comuna_y_subtipo &lt;- delitos %&gt;% filter(tipo == &quot;Homicidio&quot;) %&gt;% group_by(comuna, subtipo) %&gt;% summarise(homicidios = sum(total)) %&gt;% left_join(poblacion) head(homicidios_por_comuna_y_subtipo) Listos para facetar. Producimos un scatterplot igual que antes, y le agregamos una capa adicional con facet_wrap(). La variable a “facetar”, la que recibirá un gráfico por cada una de sus categorías, siempre se escribe a continuación del signo ~; en nuestro caso, queda como ~subtipo. El simbolillo en cuestión denota lo que en R se denomina una fórmula y ya nos lo cruzaremos de nuevo, pero por ahora no le prestamos más atención. ggplot(homicidios_por_comuna_y_subtipo) + geom_point(aes(x = poblacion, y = homicidios)) + facet_wrap(~subtipo) La notable cantidad de casos en la comuna 1 se debe a los homicidios dolosos; en ninguna otra categoría se separa tanto del resto. ¿Qué tiene de particular esta comuna? Es la más poblada, si, pero aún en proporción per cápita parece separarse de las demás (junto con otra comuna, la 4, que le sigue en población y en número de casos). Vale la pregunta, por ahora sin respuesta, para mencionar algo fundamental: por más ciencia de datos que apliquemos, siempre vamos a llegar a un punto en que nuestros hallazgos no tendrán sentido sin combinarlos con lo que se llama “conocimiento de dominio”. El conocimiento de dominio es el saber especializado sobre el tema que estamos tratando, sea el ciclo reproductivo de la gaviota austral o la organización administrativa del Gobierno de la Ciudad Autónoma de Buenos Aires. Esto no debería desanimarnos, ¡al contrario!. El análisis de datos como profesión conlleva un constante aprendizaje sobre los más variados temas. Y a la inversa: si somos expertos en cualquier campo, aún con un puñado de técnicas básicas de R podemos extraer conocimiento de nuestros datos que jamas encontraría una persona, aún experta en programación, que no conozca el contexto. 4.4 Gráficos de barras Si hay un tipo de visualización que compite en popularidad con el scatterplot, son los gráficos de barras (bar charts en inglés). Solemos encontrarlos acompañando artículos en diarios y revistas, sin duda porque son fáciles de leer de un vistazo. Los gráficos de barras se usan mucho para hacer comparaciones: quién tiene más y quién tiene menos de alguna variable continua cómo ingresos, edad, altura o similares. Comparemos la suma total de registros que alcanza cada barrio. Con geom_bar podemos agregar una capa de visualización con gráficos de barras. Los parámetros a definir dentro de aes() son x, donde va una variable categórica, y en forma opcional weight, que indica la variable a sumar para determinar la altura de cada barra. Si no especificamos un weight, simplemente se cuenta cuantas veces aparece cada categoría en el dataframe, en la práctica un conteo o frecuencia de aparición. En nuestro dataset cada fila incluye un período y un total de contactos recibidos. Nosotros no estamos interesados en cuantas veces aparece cada barrio, sino en la suma de la columna total para cada uno de ellos, así que vamos a usar weight = total. ggplot(delitos) + geom_bar(aes(x = barrio, weight = total)) Tenemos un problema. Los nombres de los barrios resultan ilegibles, porque no tienen espacio suficiente para evitar que aparezcan superpuestos. En un gráfico, el eje horizontal es un muy mal lugar para poner muchas categorías con nombre, ya que el solapamiento se vuelve inevitable. Sería mejor tener los nombre en el eje vertical, donde se pueden escribir uno encima del otro sin pisarse ¡La solución es invertir los ejes de de coordenadas! Sólo necesitamos agregar coord_flip: ggplot(delitos) + geom_bar(aes(x = barrio, weight = total)) + coord_flip() Ahora si podemos interpretar el gráfico. Además de identificar los barrios que acumulan más registros, este gráfico muestra una situación curiosa, que hasta podría indicar un error en la fuente de datos: en el barrio de La Boca aparentemente no han ocurrido delitos dignos de aparecer en el dataset. Los gráficos de barras, además de comparar, también son buenos para mostrar la composición interna de las cosas: que “hay dentro”, que componentes contribuye a un determinado total. Vamos a mostrar entonces cuanto contribuye cada tipo de trámite al total por barrio, usando el parámetro estético fill (relleno). geom_bar realiza un segmentado automático de cada barra, con la proporción que le corresponde a cada subcategoría: ggplot(delitos) + geom_bar(aes(x = barrio, weight = total, fill = tipo)) + coord_flip() Ahora se ve que robo y hurto son los tipos más frecuentes, en todos los barrios. En lugar de relleno podríamos haber usado color, tal como hicimos con los puntos, pero los resultado es un poco menos legible y no luce tan bien. La variable color modifica la silueta de las barras, pero no su interior: ggplot(delitos) + geom_bar(aes(x = barrio, weight = total, color = tipo)) + coord_flip() También podemos dejar de lado los barrios, y concentrarnos en las categorías. Si quisiéramos ver el total de registros por cada sub-tipo de incidente: ggplot(delitos) + geom_bar(aes(x = tipo, weight = total)) Notamos que los homicidios y lesiones son eventos poco frecuentes en comparación con las otras clases. En esta ocasión no recurrimos a coord_flip, ya que las categorías son pocas y tienen espacio suficiente en el eje horizontal. ¿Y si mostramos el aporte de cada barrio al total global de cada tipo de incidente? ggplot(delitos) + geom_bar(aes(x = tipo, weight = total, fill = barrio)) Hemos obtenido una visualización indigerible. ¡Son demasiadas categorías como para diferenciarlas por color! Quizás con un facetado por barrio… ggplot(delitos) + geom_bar(aes(x = tipo, weight = total)) + facet_wrap(~barrio) Esta opción es un poco mejor, ya que al menos permite identificar pronto los barrios salientes, y discernir diferencias generales si se la mira con paciencia. Una visualización tan densa en información puede resultar ideal para “uso personal”, explorando de forma rápida datos con los que estamos familiarizados, pero es poco recomendable para compartir con otros. En general, para evitar la confusión asociada a variables con docenas de categorías se busca simplificar definiendo menos grupos. Por ejemplo, como hicimos al comienzo al separar por comunas, que son sólo quince, en lugar de por barrios. 4.5 Histogramas Los histogramas son usados para mostrar la distribución de una variable continua. El histograma permite decir si los valores que toma cada observación se agrupan en torno a un valor “típico” o medio -como en el caso de la llamada distribución normal-, o en torno a dos valores frecuentes (distribución bimodal), o con dispersión sin picos ni valles, donde no hay valores típicos ni atípicos - distribución uniforme. Por ejemplo, veamos la distribución de los barrios respecto a la variable “cantidad anual de delitos”. Para contabilizar el total de registros por barrio tenemos que agrupar los valores de esa columna, y hacer un resumen (summarise()) que extraiga el gran total: delitos_anuales &lt;- delitos %&gt;% group_by(barrio) %&gt;% summarise(gran_total = sum(total)) delitos_anuales En la tabla de delistos anuales la primera fila tiene vacía la celda de barrio. Esto es porque representa los delitos para los cuales no se conoce el sitio donde ocurrieron. Es decir, muestra el total de delitos encontrados en filas donde la la variable “barrio” es NA. Hacer un histograma es simple con geom_histogram(): sólo hay que elegir una variable y asignarla a las x. ggplot(delitos_anuales) + geom_histogram(aes(x = gran_total)) geom_histogram() divide el rango de valores en una cantidad arbitraria de segmentos iguales (bins en inglés) y cuenta cuántas observaciones caen en cada uno, cantidad que se representa con la altura de la columna en el eje de las y. En nuestro ejemplo, vemos que la cantidad de registros tiende a agruparse en torno a un valor típico de bastante menos de 1.000 al mes. Aún así, par un par de barrios se contablizaron más de 4.000. No sería raro que la agregación que hicimos nos oculte patrones en los datos. Que pasa si contamos los registros por mes y por tipo de contacto, y mostramos los histogramas mensuales en facetado por tipo? Hacemos el agrupado y sumario de rigor, delitos_anuales_por_tipo &lt;- delitos_anuales &lt;- delitos %&gt;% group_by(barrio, tipo) %&gt;% summarise(gran_total = sum(total)) y creamos el facetado como ya sabemos: ggplot(delitos_anuales_por_tipo) + geom_histogram(aes(x = gran_total)) + facet_wrap(~tipo) Aparecen las diferencias. Los homicidios tienen una dispersión mínima, con casi todas las observaciones apiladas en torno a un puñado de casos; siempre son bajas. La cantidad anual de lesiones muestra una dispersión mayor, pero aún así con tendencia a rondar un valor típico. Los hurtos y robos se comportan distinto, ya que muestran una gran dispersión, pudiendo tomar cualquier valor desde una decenas a más de 2.000 registros de forma bastante pareja. 4.6 Preparando una visualización para compartir Lo último que nos queda por decir en este capítulo es que los gráficos que hemos producido hasta aquí están pensando para nuestro propio consumo. Son parte, y parte fundamental, de lo que llamamos análisis exploratorio de datos. En el contexto de la exploración, lo importante es trabajar en forma rápida, probando una u otra técnica de visualización y refinando nuestros resultados hasta hallar patrones interesantes, o sacarnos dudas acerca de los datos. No necesitamos ponerle título a las visualizaciones, porque ya sabemos de que tratan (¡acabamos de escribirlas!). No nos preocupa que los nombres de los ejes indiquen en forma clara la variable representan, porque ya lo sabemos de antemano. Pero cuando queremos guardar un gráfico para compartir con otros, sea publicándola en un paper, o enviándola por email a un amigo, necesitamos tener más cuidado. Hemos pasado del ámbito de la exploración al de la comunicación. Ahora si debe preocuparnos la claridad, porque no sabemos el grado de familiaridad que tiene con los datos la eventual audiencia. Si bien la comunicación clara es un arte cuyas reglas dependen del contexto, y además cada quien tiene su estilo, podemos decretar al menos tres elementos que no deberían faltar en un gráfico destinado a comunicar algo a los demás: Un título descriptivo, pero breve Etiquetas claras (no ambiguas) en los ejes Nombres descriptivos en las leyendas y ya que estamos, dos opcionales: Un subtítulo donde poner detalles importantes que no entran en un título breve Una nota al pie con información adicional: fuente de los datos, cita académica, advertencias, etc. Con ggplot() podemos encargarnos de todo dentro de una sola función, labs() (por labels, etiquetas) Tomemos un gráfico de los que hicimos antes para pulirlo un poco y que sirva de ejemplo. El original: ggplot(delitos) + geom_bar(aes(x = barrio, weight = total, fill = tipo)) + coord_flip() versus la versión pulida usando labs(): ggplot(delitos) + geom_bar(aes(x = barrio, weight = total, fill = tipo)) + coord_flip() + labs(title = &quot;Delitos registrados&quot;, subtitle = &quot;Ciudad Autónoma de Buenos Aires, 2020&quot;, caption = &quot;Fuente: portal de datos abiertos de la Ciudad - http://data.buenosaires.gob.ar&quot;, x = &quot;barrio&quot;, y = &quot;cantidad&quot;, fill = &quot;Tipo&quot;) Ya tiene todo el contenido necesario. Ahora si, a compartir. En cuanto a la presentación (el aspecto), podríamos hacer un ajuste final que dependerá de nuestros gustos. Los gráficos generados por ggplot() llevan ese panel gris neutro como fondo, que es característico: es fácil reconocer un gráfico hecho con ggplot al verlo en una publicación o sitio web. Sin embargo, nada nos obliga a usar el diseño definido por defecto. Podemos cambiar la tipografía, los colores usados para representar datos, el color de fondo… ¡todos y cada uno de los componentes! Una forma sencilla de hacer cambios es aprovechar los “temas” (combinaciones de colores) que el paquete ggplot2 incluye listos para usar. Para elegir un estilo sólo necesitamos agregar una capa de las que comienza con “theme_…”, por ejemplo theme_minimal(), theme_dark(), theme_classic(). En mi experiencia, el que mejor luce en la gran mayoría de los casos es theme_minimal. Vamos a usarlo, repitiendo el código que generó la última visualización que hicimos, y agregando una línea al final para definir el tema. ggplot(delitos) + geom_bar(aes(x = barrio, weight = total, fill = tipo)) + coord_flip() + labs(title = &quot;Delitos registrados&quot;, subtitle = &quot;Ciudad Autónoma de Buenos Aires, 2020&quot;, caption = &quot;Fuente: portal de datos abiertos de la Ciudad - http://data.buenosaires.gob.ar&quot;, x = &quot;barrio&quot;, y = &quot;cantidad&quot;, fill = &quot;Tipo&quot;) + theme_minimal() …no esta mal, ¿verdad? 4.7 Otras visualizaciones Por supuesto, las opciones que hemos repasado son apenas una fracción de la enorme variedad de técnicas de visualización que existen. Para empezar, nos falta hablar de los mapas, una categoría tan importante que tiene un capítulo completo dedicado más adelante. Y aún quedan tantas por discutir, que sería imposible hacerles justicia en un libro introductorio. Con nombres tan variopintos como waffle charts, violin plots, o tree maps, existen quizás un centenar o más de métodos bien documentados para explorar información en forma visual. El sitio web from Data to Viz (https://www.data-to-viz.com/) es un recurso excelente para investigar opciones. Contiene un compendio visual e interactivo de técnicas de visualización con sus nombres y características generales. También explica a que familia corresponde cada una, y para qué suele usarse (mostrar relaciones, distribuciones, cambio a través del tiempo, etc). Figura 4.1: from Data to Viz - www.data-to-viz.com/ Y lo más interesante: Para todas y cada una de las visualizaciones se incluye el código en R que permite reproducirlas. A partir de allí sólo es cuestión de adaptar los ejemplos a nuestros datos para realizar de forma fácil la visualización que nos interesa. 4.8 Ejercicios I. Tomemos un dataset, y efectuemos las transformaciones necesarias para luego crear las siguientes visualizaciones: Scatter plot, o gráfico de dispersión: Mostrar correlación entre 2 variables numéricas. Gráfico de Barras: Comparar variables categóricas. Gráfico de Barras: Comparar variables categóricas mostrando la composición interna de las mismas. Histograma: Mostrar la distribución de una variable continua. Realicemos una versión facetada (con facet_wrap()) de cada tipo de gráfico. Pongamos a cada visualización su título, subtítulo, etiquetas en los ejes, nombre descriptivo en la leyenda y nota al pie. "],["analisis-de-texto.html", "5 Analisis de texto 5.1 Manipulación de texto 5.2 Metacaracteres 5.3 Primer ejercicio 5.4 Ejercicios", " 5 Analisis de texto A veces, lo que queremos analizar es un texto. Los textos, audios e imágenes son datos no estructurados y hay que realizar algún tipo de preparación o estructuración para poder procesarlos automáticamente. En el análisis automático de texto se suelen partir los documentos, y armar lo que se llama un bolsa de palabras, donde se rompe la sintaxis . Ello sirve para medir frecuencias de palabras, cercanía entre ellas, clasificarlas como “positivas” o “negativas” sentimentalmente hablando, etc. La minería de texto tiene diversas herramientas para realizar este tipo de tareas. Las palabras, por su parte, son cadenas de caracteres, y también se pueden manipular de modo automático. Para hacer ello usaremos lo que se conoce como expresiones regulares (“regular expressions” o regex) que permitirá detectar patrones y manipularlos. Las regex son relativamente antiguas (circa 1950) y tuvieron un desarrollo paralelo a la ciencia de datos. Por ello, vienen en varios sabores, cada uno sutilmente diferente al otro. R utiliza regexa través de varios paquetes, entre ellos el stringr de la constelación tidyverse. 5.1 Manipulación de texto En stringr todas las funciones comienza con str_. Usando RStudio, si se presionamos la tecla tab luego de escribir str_ vamos a ver una lista de funciones sugeridas. Por ejemplo, para unir -o concatenar- caracteres se utiliza str_c: library(tidyverse) oracion &lt;- c(&quot;Esta&quot;, &quot;es&quot;, &quot;una&quot;, &quot;cadena&quot;, &quot;de&quot;, &quot;palabras.&quot;) oracion ## [1] &quot;Esta&quot; &quot;es&quot; &quot;una&quot; &quot;cadena&quot; &quot;de&quot; &quot;palabras.&quot; # con `collapse = &quot; &quot;` indicamos que queremos combinar los elementos y separarlos con un espacio oracion &lt;- str_c(oracion, collapse = &quot; &quot;) oracion ## [1] &quot;Esta es una cadena de palabras.&quot; Para extraer caracteres de una palabra se utiliza str_sub. Hay que señalar el comienzo y el final de lo que se quiera extraer, con el número de la posición: frutas &lt;- c(&quot;Manzana&quot;, &quot;Banana&quot;, &quot;Pera&quot;) str_sub(frutas, 1, 3) ## [1] &quot;Man&quot; &quot;Ban&quot; &quot;Per&quot; str_sub(frutas, -3, -1) ## [1] &quot;ana&quot; &quot;ana&quot; &quot;era&quot; Se puede pasar los caracteres a mayúscula, o a minúscula: str_to_lower(frutas) ## [1] &quot;manzana&quot; &quot;banana&quot; &quot;pera&quot; str_to_upper(frutas) ## [1] &quot;MANZANA&quot; &quot;BANANA&quot; &quot;PERA&quot; str_to_sentence(str_c(frutas, collapse = &quot; &quot;)) ## [1] &quot;Manzana banana pera&quot; Una función muy importante nos permite detectar patrones, str_detect, que nos dice si es verdadero o falso que dicho patrón está en el objeto. Para verlo más en detalle, utilizaremos str_view para que la señale en el texto cuando la detecte: frutas ## [1] &quot;Manzana&quot; &quot;Banana&quot; &quot;Pera&quot; str_view(frutas, &quot;an&quot;) Y ahora es donde comienzan a ponerse interesantes las regex. Por ejemplo, el punto (.) puede reemplazar a cualquie caracter, excepto el salto de línea str_view(frutas, &quot;.an&quot;) Pero si el “punto” reemplaza a cualquier caracter, ¿cómo seleccionamos al caracter “.”? Tenemos que utilizar un “escape” que le diga que estamos refiriendo al caracter y no al regex. para ello, se usa la barra invertida \\, que se denomina escape. Pero, y entonces, ¿cómo difereciamos al escape de la barra invertida?: Para que sepa que es un escape y no una barra invertida, debemos escaparla, de modo que serán dos barras invertidas, más el caracter que queremos escapar… oracion ## [1] &quot;Esta es una cadena de palabras.&quot; str_view(oracion, &quot;.as\\\\.&quot;) saludo &lt;- c(&quot;Quiero saludar a los Sres. padres y las Sras. madres&quot;) str_view(saludo, &quot;Sr.s\\\\.&quot;) str_view_all(saludo, &quot;Sr.s\\\\.&quot;) #para que detecte todas las coincidencias y no solo la primera. 5.2 Metacaracteres 5.2.1 Comienzo y fin de línea -^para buscar solo al comienzo de la línea - $ para buscar solo al final de la línea x &lt;- c(&quot;arándano&quot;, &quot;banana&quot;, &quot;pera&quot;) str_view(x, &quot;^a&quot;) str_view(x, &quot;a$&quot;) 5.2.2 Clases de Palabras Cuando se usan los corchetes, se pueden coincidir una de varias opciones. Mientras que a identifica una “a” y e identifica una “e”, [ae] identifica ya sea una “a” o una “e”. Esto es muy útil para las dudas ortográficas… x &lt;- &quot;Esto es muy necesario, o nesesario?&quot; str_view_all (x, &quot;ne[cs]e[cs]ario&quot;) Entre corchetes, se pueden listar muchas clases de caracteres. Por ejemplo: x &lt;- &quot;Telefono: 3321-4430&quot; str_view(x, &quot;[0123456789]&quot;) str_view(x, &quot;[0-9]&quot;) # el guión es un metacaracter que indica rango str_view(x, &quot;[0-9]{8}&quot;) # ¿ por qué no selecciona los ocho números? str_view(x, &quot;[0-9]{4}&quot;) # ahora solo cuatro... str_view(x, &quot;[0-9]{4}.[0-9]{4}&quot;) #vean el punto entre los grupos de números (telefono &lt;- str_extract(x, &quot;[0-9]{4}.[0-9]{4}&quot;)) ## [1] &quot;3321-4430&quot; Con str_extract() asignamos a la variable telefono el número que leimos automáticamente. Si esto está en un mail o formulario, o algún otro texto, puedo ejecutar el código y leer a través del regex lo que dice. Claro que para ello hay que conocer el tipo de texto que se trata: tengo que saber que el número de teléfono son ocho dígitos separados por un guión. Cuando sé que tipo de patrón estoy buscando, puedo armar un regex para leerlo y procesarlo automágicamente. Y si queremos identificar todos los números de telefono de una planilla, también lo podemos hacer de modo automático. listado &lt;- tibble(nombre = c(&quot;carlos&quot;, &quot;laura&quot;, &quot;pedro&quot;, &quot;maria&quot;, &quot;juan carlos&quot;, &quot;miguel&quot;, &quot;teresa&quot;), telefono = c(&quot;4323-3341&quot;,&quot;4664-9800&quot;, &quot;4121-9073&quot;, NA, &quot;4112-5440&quot;, &quot;3442-1009&quot;, NA)) listado telefonos &lt;- str_extract_all(listado, &quot;[0-9]{4}.[0-9]{4}&quot;) %&gt;% unlist() telefonos ## [1] &quot;4323-3341&quot; &quot;4664-9800&quot; &quot;4121-9073&quot; &quot;4112-5440&quot; &quot;3442-1009&quot; Hay otros metacaracteres muy útiles. [a-z]es una secuencia de todas las letras en minúscula; [A-Z] en mayúscula. Para elegir todos los dígitos, lo podemos hacer con \\\\d, y todos los no dígitos con \\\\D. Con \\\\w todos los caracteres alfanuméricos ([a-zA-Z0-9_]) y con \\\\W todos los no alfanuméricos (símbolos, puntos, etc). Con \\\\s podemos elegir todos los espacios en blanco. Luego, un [^ ] niega lo anterior: [^a-z] matchea lo que no tenga alguna letra (Ojo, ^ actúa distinto si está dentro o fuera de los corchetes. Fuera es un ancla de inicio de línea, dentro es negación de lo siguiente). Una que es muy importante es la alternación |, donde matchea una expresión u otra: \"(Julio|Jul)\" para detectar tanto cuando dice “Julio” o si dice “Jul”. Los paréntesis se usan igual que en matemática, para encerrar conceptos y tratarlos como un único concepto (por ejemplo, para que comience con Julio o Jul puedo poner: ^(Julio|Jul). Sin los paréntesis, sólo buscaría que no comience con “J”). También hay otros metacaracteres para indicar repeticiones: ? repite el anterior una vez: .? será cualquier caracter una vez, o ninguno (es opcional). Luego + repetirá una vez al menos y el resto opcional. Y * repetirá muchas veces de modo opcional. Como vimos, en este sabor de regex, un número entre corchetes dice exacto cuántas veces se debe repetir el patrón: {4}. El listado es el siguiente: Metacaracter Nombre Selecciona \\d Dígito un dígito \\s Espacio en blanco cualquier espacio en blanco incluyendo espacios, tabuladores, saltos de línea, [a-z,A-Z] Rango de letras cualquier letra en el rango especificado . Punto cualquier caracter […] Clase de caracteres cualquier caracter de la clase [^…] Negativo de clase de caracteres cualquier caracter que no esté en la clase ? Signo de interrogación Uno permitido, pero es opcional * Asterisco Cualquier cantidad permitida, pero toda son opcional + Más Al menos uno es requerido, más son opcionales | Alternativa Selecciona una expresión o la otra que separa ^ Caret Selecciona la posición al comienzo de la línea $ Peso Selecciona la posición al final de la línea {X,Y} Rango específico X es requerido, máximo Y permitidos 5.2.3 Lookaround Los lookaround nos permiten identificar una posición, y no texto. Luego de identificar esa posición, podemos buscar texto. Esta posición puede ser tanto en referencia a la derecha (adelante, lookahead (?= )) como a la izquierda (atras, lookbehind (?&gt;= )). Entonces deberíamos pedirle a regex que identifique el lugar desde donde queremos seleccionar algún patrón. Para ello podemos incluso combinarlos: despues de tal patrón y antes de este otro, y luego decirle con qué queremos hacer el match. discurso &lt;- &quot;Sr. Presidente (Gioja).- Corresponde ahora pasar al tiempo destinado a los representantes de los bloques. En primer lugar, el Frente de Todos. Tiene la palabra la señora diputada Aparicio, por Buenos Aires. Sra. Aparicio.- Señor presidente: hoy debatimos de cara a la sociedad, con responsabilidad y transparencia, como nunca se lo ha hecho en este Congreso, la triste historia de procesos de endeudamiento. Sr. Allende.- Señor presidente: quiero destacar la posibilidad que tenemos de analizar este acuerdo con elFondo. &quot; str_view_all(discurso, &quot;(?&lt;=Sr.?\\\\.\\\\s)(?=\\\\w)\\\\w*&quot;) 5.3 Primer ejercicio La desigualdad estructural de género se manifiesta también en las mayores dificultades que tienen las mujeres para acceder a posiciones de poder. Para comenzar a remediar esto, en diversos países se adoptaron medidas de acción positiva, como las leyes de cupo femenino para garantizar un determinado porcentaje mínimo de diputadas mujeres en relación con los varones. Así, en Argentina, en el año 1991 se adoptó una primera ley que estableció la obligatoriedad de garantizar una mujer entre las primeros tres personas candidatas en las listas de diputados y así sucesivamente, para propender a alcanzar un cupo mínimo del 30% de las bancas en disputas para las mujeres. Recientemente, en el 2017 se aprobó una ley de paridad, en donde se debe garantizar el 50% de las bancas para las mujeres. Una investigadora tuvo la idea de medir esta participación en la práctica, y para ello contó cuántas veces tomó la palabra una mujer y cuántas veces un varón, a partir de la implementación de la ley de paridad. En este ejercicio vamos a intentar medir esto con los regex. Antes de continuar, vamos a instalar un nuevo paquetes de funciones: pdftools, que permite extraer el texto de archivos en formato PDF. Para instalarlo usamos install.packages(), tal como hicimos antes para instalar otros paquetes. install.packages(&quot;pdftools&quot;) Recordemos que sólo hace falta instalar paquetes una vez. Es decir, habiendo ejecutado con éxito la línea install.packages(\"pdftools\") ya no hace falta volver hacerlo la próxima vez que necesitemos recurrir al paquete. Ya quedó instalado en nuestro sistema 5.3.1 Consiguiendo los datos La página de la Cámara de Diputados de Argentina tiene una pequeña sección de datos abiertos, en https://datos.hcdn.gob.ar/. De allí podemos descargar un dataset con todas las sesiones que hubo, por período parlamentario. library(pdftools) sesiones &lt;- read.csv(&quot;https://cdaj.netlify.app/data/sesiones/sesiones.csv&quot;) Una ventaja de la programación es la reproducibilidad. Y para ello, en general, es útil descargar las bases de datos directamente desde las paginas web. Sin embargo, los links muchas veces cambian y las paginas cuando menos lo esperamos, caen. Por ello, siempre es mejor tener una copia de la base en nuestra PC o nuestra nube y referenciarla de allí. Para este ejercicio utilizaremos solamente el período del año 2020, que es el número de sesiones 138. Del sitio web de la Cámara de Diputados podemos, o bien vincular las versiones taquigráficas con el link, o bien descargarlas a la PC. Con ello, podremos cargarlas en nuestra tabla. Una vez descargadas o identificados los links, agregamos una columna con el link para luego descargar el texto. periodo138 &lt;- sesiones %&gt;% filter(str_detect(periodo_id, &quot;138&quot;), reunion_tipo != &quot;Apertura Ordinarias&quot;) periodo138 &lt;- periodo138 %&gt;% mutate(reunion_nombre = paste0(&quot;138-&quot;, reunion_numero)) periodo138 &lt;- periodo138 %&gt;% mutate(link = paste0(&quot;https://cdaj.netlify.app/data/sesiones/&quot;, reunion_nombre, &quot;.pdf&quot;)) Nuestra lista de sesiones, ahora con una columna con el link a su transcripción, luce así: periodo138 Y ahora leemos los documentos y los agregamos a nuestra tabla. Las celdas no tienen limite máximo de capacidad, y entonces colocamos cada versión taquigráfica en una celda en la fila de la sesión respectiva. Para hacer esto usaremos la función map para que la función que lee PDFs (pdf_text) se ejecute cada vez en cada fila de nuestro dataframe. periodo138 &lt;- periodo138 %&gt;% mutate(texto = map(link, pdf_text)) # puede demorar algunos minutos! Ojo. Ahora en la tabla hay mucha información, lo que resulta pesado para las computadoras. Puede ser que si queremos visualizar la tabla se cuelgue la computadora o se haga muy lenta. Entonces, habrá que evitar abrir la tabla entera. Mejor es llamar a las celdas individualmente desde la consola 5.3.2 Limpieza de datos El texto tiene muchos caracteres que son parte del formato, y que deberíamos limpiar de modo previo a hacer el análisis. La etapa de limpieza debe tener en cuenta cuál es el objetivo de los datos y del análisis. Para nuestro ejercicio, en tanto vamos a medir cuántas veces toman la palabra los diputados y las diputadas, deberíamos eliminar cada vez que toma la palabra el Sr. Presidente o el Sr. Secretario. Eliminamos también todos los saltos que figuran como \\\\n (recuerden que para seleccionar una barra tenemos que escaparla dos veces). limpio &lt;- periodo138 %&gt;% mutate(texto = str_replace_all(texto, &#39;\\\\s|\\\\n|\\\\\\\\n|\\\\\\&quot;&#39;, &quot; &quot;)) limpio &lt;- periodo138 %&gt;% mutate(texto = str_remove_all(texto, &quot;Sr. Presidente|Sr. Secretario|Sra. Presidenta&quot;)) rm(periodo138) #remuevo el objeto para liberar un poco de memoria 5.3.3 Análisis En este ejercicio sólo queremos contar cuántas veces toma la palabra una diputada mujer y cuántas uno varón. Afortunadamente, en la versión taquigráfica, cada vez que comienza a hablar un diputado varón lo refieren como “Sr. xxxx” y cada vez que comienza a hablar una diputada mujer la refieren como “Sra. yyyy”. Por ello, nos alcanza con contar cuántas veces dice “Sr.” y cuántas “Sra.”. cantidad &lt;- limpio %&gt;% mutate(varon = str_count(texto, &quot;Sr.&quot;), mujer = str_count(texto, &quot;Sra.&quot;), ratio = round(varon/mujer,3)) %&gt;% select(-reunion_numero, -sesion_numero,-texto, -link, -sesion_camara, -periodo_id, -reunion_fin) cantidad Y si queremos ver el promedio, calculamos mean(cantidad$ratio) ## [1] 3.216053 Esto significa que en promedio, durante el período de estudio, los varones hablaron 3 veces más que las mujeres. 5.3.4 Quienes hablaron? Y también podemos analizar quiénes tomaron la palabra en cada sesión. Con el código de lookaround que usamos arriba, podemos agregar una columna “toman_palabra” que contenga una lista con los nombres que se registran tomando la palabra en cada sesión: quienes &lt;- limpio %&gt;% mutate(toman_palabra = str_extract_all(texto, &quot;(?&lt;=Sr.?\\\\.\\\\s)(?=\\\\w)\\\\w*&quot;)) %&gt;% select(-reunion_numero, -sesion_numero,-texto, -link, -sesion_camara, -periodo_id, -reunion_fin) quienes Notamos que el contenido de la nueva columna es un tipo de dato complejo: un lista (list) de cadenas de texto. Para ver el contenido de estas listas podemos extraerlas con quienes$toman_palabra, y eso va a generar un montón de salida de texto en pantalla. O para algo más acotado, podemos ver sólo el contenido de la fila n (por ejemplo, la fila 3 con quienes$toman_palabra[3]). O podemos usar filter() para ver el contenido de la lista en las filas que cumplan alguna condición. Por ejemplo: quienes %&gt;% filter(reunion_nombre == &quot;138-16&quot;) %&gt;% pull(toman_palabra) ## [[1]] ## [1] &quot;Moreau&quot; &quot;Moreau&quot; &quot;Moreau&quot; &quot;Ritondo&quot; &quot;Negri&quot; &quot;Negri&quot; ## [7] &quot;Ramón&quot; &quot;Álvarez&quot; &quot;Sapag&quot; &quot;Ávila&quot; &quot;Quetglas&quot; &quot;Ritondo&quot; ## [13] &quot;Álvarez&quot; &quot;Negri&quot; &quot;Ramón&quot; &quot;Sapag&quot; &quot;Ávila&quot; &quot;del&quot; ## [19] &quot;Cano&quot; &quot;González&quot; &quot;Zottos&quot; &quot;Del&quot; &quot;Frade&quot; &quot;El&quot; ## [25] &quot;Camaño&quot; &quot;Iglesias&quot; &quot;Álvarez&quot; &quot;Bernazza&quot; &quot;Cerruti&quot; (la función pull() extrae el contenido de una columna dada, tiene el mismo efecto que el simbolillo \"\\(&quot; en `quienes\\)toman_palabra`) Ahora podemos crear una columna que contenga conteos de la cantidad de veces que aparece cada nombre tomando la palabra, en cada fila. Esta vez el contenido de cada item en la nueva columna será una tabla de frecuencias, que en R se representan con su propio tipo de datos: quienes &lt;- quienes %&gt;% mutate(frecuencia = map(toman_palabra, table)) quienes Para revisar estas tablas de frecuencia, podemos usar cualquiera de los métodos mencionados antes, como: quienes %&gt;% filter(reunion_nombre == &quot;138-16&quot;) %&gt;% pull(frecuencia) ## [[1]] ## ## Álvarez Ávila Bernazza Camaño Cano Cerruti del Del ## 3 2 1 1 1 1 1 1 ## El Frade González Iglesias Moreau Negri Quetglas Ramón ## 1 1 1 1 3 3 1 2 ## Ritondo Sapag Zottos ## 2 2 1 5.4 Ejercicios En la última tabla pudimos ver en concreto quién habló y cuántas veces lo hizo. Pero notemos que aparecen algunos “del” y “de” sueltos… ¡falta el resto de esos apellidos!. ¿Cómo podríamos obtener el apellido completo de \"DE LA SOTA, NATALIA\", \"DE LOREDO, RODRIGO\", \"DEL PLA, ROMINA\", etc? Descarguemos un tomo de jurisprudencia de la Corte Suprema de Justicia de la Nación (https://sjservicios.csjn.gov.ar/sj/tomosFallos.do?method=iniciar), o la similar en otros países, y extraigamos las citas a precedentes: \"Fallos xxx:xxx\" "],["trabajando-con-datos-espaciales-y-temporales.html", "6 Trabajando con datos espaciales y temporales 6.1 Trabajando con fechas 6.2 Mirando al espacio 6.3 Combinando espacio y tiempo 6.4 Ejercicios", " 6 Trabajando con datos espaciales y temporales Hasta hace poco tiempo, labores como la producción de mapas y el análisis espacial estaban reservadas para especialistas. La complejidad de las tareas y al alto costo de producción y adquisición de datos geográficos funcionaban como barrera difícil de superar. Pero durante las dos últimas décadas la tecnología digital cambió el panorama. Una brusca caída en el costo asociado a adquirir y procesar información geográfica (pensemos en satélites y computadoras multiplicándose y bajando de precio) dio paso al mapa digital como herramienta universal. El consumo de sofisticados mapas y otros productos geográficos se volvió masivo y cotidiano, con Google Maps como el exponente más conocido. Apenas había pasado la novedad de disponer de mapas en alta resolución de todo el mundo accesibles al instante desde nuestros escritorios, cuando la llegada de los smartphones popularizó el acceso en cualquier momento y lugar. El mismo proceso que nos convirtió a todos en consumidores constantes de información geográfica también nos da la oportunidad de ser los productores. Sin dudas, hacer mapas se ha vuelto más fácil que nunca antes. Existen cada vez más repositorios con información georreferenciada de acceso publico, datasets que incluyen información precisa sobre la ubicación geográfica de sus registros. Al mismo tiempo, maduran y se hacen más fáciles de usar las herramientas para análisis y visualización espacial. En los procesos sociales, el “dónde” suele ser un aspecto clave. Es central para quienes estudiamos -por ejemplo- las ciudades o las dinámicas de la política, siempre tan arraigadas a lo territorial. Esto vuelve al mapa una de las herramientas de visualización más importantes que podemos emplear. Del mismo modo, también crece la disponibilidad datos que incluyen fecha y hora. Para entender datasets con datos en gran volumen que poseen atributos de posición y tiempo, es útil visualizar el ritmo en el que ocurren (diario, mensual, anual, etc) tanto como la forma en la que se distribuyen en el espacio. 6.1 Trabajando con fechas Continuemos aprovechando el dataset con delitos registrados durante el 2020 en Buenos Aires, ahora para practicar el manejo de fechas y horas. Esta vez usaremos una versión aún más completa, que incluye columnas con la fecha completa, la hora, y la ubicación de cada incidente: delitos &lt;- read.csv(&quot;https://cdaj.netlify.app/data/delitos_fecha_lugar.csv&quot;) Las primeras filas del dataset lucen así (: Teniendo en cuenta que el dataset es del 2020, el Primer Año de la Pandemia COVID, nos podríamos preguntar si se ve algún descenso brusco en robos, hurtos, homicidios o siniestros viales con el inicio de la cuarentena. Para responder eso, vamos a tener que lidiar con datos de tipo “fecha”. Esto podría ser bastante engorroso, pero por suerte podemos usar una herramienta que simplifica las cosas. 6.1.1 La fecha como clase de variable La fecha es un tipo de dato que puede ser expresado de muchas maneras, dependiendo de que nos interese tener en cuenta: el día de la semana al que corresponde, el mes, el año, etc. El paquete lubridate hace fácil tomar una variable que contiene fechas en cualquier formato (por ejemplo “20/07/2020”) para extraer el atributo relacionado que deseemos (como su día, “lunes”, o su mes, “julio”). Para empezar, convertimos el campo “fecha” al tipo de dato especializado, que se llama… fecha (date). Aquí tenemos que prestar atención al formato en que aparecen los datos de la columna, en general algo como “30-07-2018” (día, mes, año) o “30-07-2018 12:14:24” (día, mes, año y hora, minutos, segundos). Con nuestros datos se da el primer caso, por lo cual la función para convertir ese campo en fecha es dmy(); para el segundo caso, seria dmy_hms() library(lubridate) delitos$fecha &lt;- dmy(delitos$fecha) lubridate usa un formato memotécnico para nombrar sus funciones de conversión de texto a fecha. Para saber que función utilizar según el formato en que aparecen las fechas, es cuestión de determinar en que orden aparecen año, mes y día (y horas y minutos si estuvieran). Por ejemplo, un campo que registran fechas en formato similar a “4/26/17” o a “4-26-2017” implica “mes - día - año” y por tanto en ambos casos puede convertirse a fecha con mdy() (la “y” es por year). Repasemos algunas de los nuevos trucos que podemos hacer con el tiempo. Tomemos cinco fechas elegidas al azar: muestra_de_fechas &lt;- sample(delitos$fecha, 5) muestra_de_fechas ## [1] &quot;2020-12-07&quot; &quot;2020-11-05&quot; &quot;2020-09-21&quot; &quot;2020-09-27&quot; &quot;2020-02-21&quot; Mediante las funciones disponibles en lubridate, podemos extraer: El día de la semana al que corresponde cada fecha: wday(muestra_de_fechas) ## [1] 2 5 2 1 6 wday(muestra_de_fechas, label = TRUE) ## [1] lun jue lun dom vie ## Levels: dom &lt; lun &lt; mar &lt; mié &lt; jue &lt; vie &lt; sáb El mes: month(muestra_de_fechas) ## [1] 12 11 9 9 2 month(muestra_de_fechas, label = TRUE) ## [1] dic nov sep sep feb ## 12 Levels: ene &lt; feb &lt; mar &lt; abr &lt; may &lt; jun &lt; jul &lt; ago &lt; sep &lt; ... &lt; dic El año: year(muestra_de_fechas) ## [1] 2020 2020 2020 2020 2020 Y varias opciones más, que se pueden repasar en https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html Con lo visto hasta aquí, tenemos suficiente para mostrar patrones temporales en los datos. Empecemos por un gráfico de barras (geom_bar()) con la cantidad de eventos registrados por mes. Para que aparezca un conteo por mes del año, asignaremos al eje de las \\(x\\) el mes al que corresponde cada valor de la columna “fecha”, o sea month(fecha, label = TRUE): library(tidyverse) # cargamos la librería en caso de no haberlo hecho antes ggplot(delitos) + geom_bar(aes(x = month(fecha, label = TRUE))) (Hemos usado el parámetro label = TRUE para obtener el nombre del mes en lugar de su número, o sea “dic” en lugar de “12”. Prueben realizar el gráfico sin ese parámetro en la llamada a month() para ver que pasa.) En el gráfico, se ve una reducción drástica a partir de la cuarentena decretada en abril. Y también un incremento gradual y sostenido a partir de allí, que de todos modos no llega a los niveles pre-pandemia de enero y febrero. Para ver la composición interna de los conteos mensuales, cuantos casos corresponden a cada categoría, podemos hacer un gráfico de “barras apiladas” como vimos en la clase 3. La sintaxis es igual que antes, pero esta vez asignamos la variable tipo\"_ al color de relleno de las barras determinado por el parámetro fill. ggplot(delitos) + geom_bar(aes(x = month(fecha, label = TRUE), fill = tipo)) Las barras apiladas son prolijas, pero pueden hacer difícil evaluar la evolución de categorías individuales. Recordemos que para mostrar los subconjuntos en barras independientes, una al lado de la otra, podemos usar el parámetro position = \"dodge\". Inténtenlo agregando el parámetro al ejercicio anterior para ver como queda. Ahora comparemos la cantidad de eventos registrados, por tipo, para cada día de la semana. Basta con usar la función que extrae el día de la semana, wday(), en lugar de month(). Lo demás es idéntico, incluyendo el uso de label = TRUE para que obtener el nombre del día -“lun”-, en lugar de su posición en la semana -“2”, porque para lubridate las semanas empiezan el domingo-. ggplot(delitos) + geom_bar(aes(x = wday(fecha, label = TRUE), fill = tipo)) Como era de esperar, durante los fines de semana se observa una menor cantidad de eventos, aunque quizás no en la categoría Homicidio, que es difícil de discernir por su relativa escasez. Para solucionar el problema podríamos filtrar los datos antes de visualizarlos, como hemos hecho antes con la función filter(), mostrando sólo la categoría de interés. Otra opción, que probaremos ahora, es una visualización en “facetas”. Se trata de generar una visualización que muestra distintos aspectos de los datos en paneles separados. Con un ejemplo vamos a dejarlo mas claro. Para realizar un gráfico en facetas, basta con sumar una línea con la función facet_wrap(vars(x, y, ...)), dónde “x”, “y”, etc son los nombres de las variables cuyas categorías recibirán paneles distintos. Intentémoslo con el último gráfico, agregando “tipo” como variable a facetar: ggplot(delitos) + geom_bar(aes(x = wday(fecha, label = TRUE), fill = tipo)) + facet_wrap(vars(tipo)) Obtuvimos barras separadas, pero los homicidios siguen difíciles de distinguir. Se debe a que por defecto facet_wrap() mantiene a escala todos los paneles, de manera que se puedan comparar cantidades de forma directa. La desventaja es que se pierde legibilidad de categorías con valores ínfimos en comparación con otras. Por eso disponemos del parámetro scales, que permite graficar los datos con escala libre, vía scales = \"free\". Probemos: ggplot(delitos) + geom_bar(aes(x = wday(fecha, label = TRUE), fill = tipo)) + facet_wrap(vars(tipo), scales = &quot;free&quot;) Ahora queda más claro que los homicidios siguen un patrón diario distinto al de las demás categorías. También que, en términos relativos, las lesiones son las que mas se reducen durante los fines de semana. También podemos evaluar el ritmo según la hora del día. ¿Cómo se haría con nuestro dataset? 6.2 Mirando al espacio Pasemos ahora al análisis espacial de nuestros datos. Para facilitar la visualización volveremos a usar el paquete ggmap, que aporta funciones que facilitan la creación de mapas. library(ggmap) 6.2.1 Obteniendo un mapa base Para dar contexto que guíe a la audiencia es útil proyectar nuestra información georreferenciada sobre un mapa de fondo que muestre la posición de caminos, nombres de localidades, accidentes geográficos y otros hitos. Estos mapas usados como lienzo sobre el cual mostrar datos espaciales son conocidos como “mapas base”, o basemaps. Guardar por nuestra cuenta información cartográfica con alto nivel de detalle de cualquier lugar del mundo es impracticable, pero por suerte existen servicios en internet que lo hacen por nosotros como Stamen Maps, ¡que es de uso gratuito!. El sitio web de Stamen Maps Para incluir mapas base en nuestras visualizaciones podemos usar las funciones del paquete ggmap, que complementa a ggplot agregando funciones que permiten adquirir y visualizar mapas en forma fácil. Lo activamos: library(ggmap) Ahora, para obtener un mapa base del área donde se encuentran los datos que queremos mostrar, necesitamos determinar su “bounding box”: el rango de latitud y longitud que forma un rectángulo conteniendo todas sus posiciones. En resumidas cuentas, se trata de los valores de latitud máxima y mínima, y de longitud máxima y mínima, de nuestros datos georreferenciados. Los proveedores de mapas online suelen solicitar los valores en este orden: izquierda, abajo, derecha, arriba. Es decir, posición mas al oeste, posición mas al sur, posición mas al este, posición mas al norte. Cuando disponemos de un daataframe georreferenciado, obtener la bounding box de su contenido es bastante fácil usando st_bbox, una función del paquete sf que recupera esas cuatro coordenadas clave. Por ejemplo st_bbox(radios) obtiene las coordenadas del rectángulo de territorio que abarca los radios censales de Buenos Aires. Luego de guardar las coordenadas en una variable, les ponemos los nombres que permitirán que ggmap las identifique: Para obtener un mapa de fondo o “mapa base” necesitamos obtener una bounding box de nuestros datos: el rango de latitud y longitud que forma un rectángulo conteniendo todas sus posiciones. En resumidas cuentas, se trata de los valores de latitud máxima y mínima, y de longitud máxima y mínima, de nuestros datos georreferenciados. No es algo que deba preocuparnos demasiado, porque podemos obtener esta información con una simple función incluida en ggmap: bbox &lt;- make_bbox(delitos$longitud, delitos$latitud) bbox ## left bottom right top ## -58.54046 -34.71247 -58.33403 -34.52172 En base a la “bounding box” solicitamos nuestro mapa base: CABA &lt;- get_stamenmap(bbox = bbox, maptype = &quot;toner&quot;, zoom = 12) Para verlo: ggmap(CABA) Stamen ofrece varios estilos de mapa base, que pueden revisarse en su sitio web. Entre ellos: terrain (usado por defecto) toner (monocromático, buena opción para visualizar datos proyectados por encima) toner-lite (versión alternativa de toner, con menos contraste visual) watercolor (hay que probarlo para apreciarlo, pero digamos que es artístico) Se adquieren usando el parámetro maptype. Por ejemplo, get_stamenmap(bbox = bbox, maptype = \"watercolor\", zoom = 12) 6.2.2 De coordenadas al mapa De aquí en más podemos suporponer nuestros datos en distintas capas sobre el mapa, con la misma sintaxis que conocemos de ggplot. Para mapear las ubicaciones de los delitos en el dataset, usamos geom_point() y los campos de longitud y latitud para los ejes \\(x\\) e \\(y\\): En ocasiones los datos de longitud y latitud son leídos como texto (tipo “chr”), en lugar de números. Si eso ocurre el mapeo no va a funcionar, porque necesita datos numéricos para latitud y longitud. Para solucionarlo, podemos intentar convertir las columnas de coordenadas a tipo numérico. Algo así como misdatos$longitud &lt;- as.numeric(misdatos$longitud) ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud)) Aquí nos topamos con un problema habitual al trabajar con grandes volúmenes de datos. Hay tantos puntos proyectados sobre el mapa, que se hace imposible interpretar dónde existen más o menos. Hacemos algunos ajustes: un color que resalte más contra el mapa base, y que no se confunda con él un tamaño de punto más pequeño y aplicación de una ligera transparencia Todo ello vía los atributos “color”, “size” y “alpha”. ¿Cuál es el valor ideal para cada uno? En general, hay que recurrir a la prueba y error para encontrar la receta justa. Probemos con color = \"orange\", size = 0.1 y alpha = 0.1: ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud), color = &quot;orange&quot;, size = 0.1, alpha = 0.1) Ahora si aparecen ciertos patrones, por ejemplo la mayor frecuencia de casos de casos cerca de las principales de circulación de la ciudad. Aún así, se hace difícil identificar de un golpe de vista las “zonas calientes”, las áreas de máxima concentración. 6.2.3 Mapas de densidad Una solución práctica para el problema de la cantidad de puntos es una técnica llamada “binning”: dividir el espacio en una grilla de celdas, contar cuantos puntos caen dentro de cada una, y visualizar las cantidades agregadas. En el mundo ggplot esto se lleva a cabo con geom_bind2d(). ggmap(CABA) + geom_bin2d(data = delitos, aes(x = longitud, y = latitud)) Ahora si, resaltan las áreas de mayor concentración de incidentes. Se puede mejorar un poco el gráfico usando: una mayor cantidad de celdas para aumentar la resolución una escala de colores diseñada para ayudar a detectar diferencias por tonalidad, como Viridis. la cantidad de celdas se define con el parámetro “bins”, por ejemplo bins = 100. La escala de color Viridis, como ya habíamos visto, se agrega sumando una llamada a scale_fill_viridis_c() -porque aquí la data es continua, si fuera discreta usaríamos scale_fill_viridis_d(). ggmap(CABA) + geom_bin2d(data = delitos, aes(x = longitud, y = latitud), bins = 100) + scale_fill_viridis_c() Una alternativa al binning es la llamada kernel density estimation, muy utilizada en aplicaciones de análisis espacial para estimar la intensidad de una determinada variable en cualquier punto del área analizada, incluso en aquellos para los cuales no hay observaciones. La idea es asumir que los valores observados corresponden a una distribución continua sobre el espacio, y determinar cual es la más probable en base a los puntos donde existen datos. Podemos visualizar esta distribución estimada geom_density2d_filled así: ggmap(CABA) + geom_density2d_filled(data = delitos, aes(x = longitud, y = latitud), alpha = 0.5) Nótese que aplicamos transparencia usando el parámetro alpha = 0.5. ¿Por qué? ¿Qué pasa si lo quitamos? 6.2.4 Visualizando multiples categorías Hasta aquí hemos analizado la distribución espacial de eventos en su totalidad, sin diferenciar su tipo. Veamos ahora las diferencias por categoría. Podemos reintentar el mapa de puntos, esta vez diferenciándolos por color. Recuperamos el código que usamos antes para mostrar puntos, y esta vez asignamos la columna “tipo” al atributo estético color: ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud, color = tipo), size = 0.1, alpha = 0.1) Aquí tenemos dos problemas: La leyenda (“tipo_delito”) es difícil de leer, dado que muestra los puntos tal como los definimos: pequeños y con mucha transparencia. Esos atributos son útiles en el mapa, donde tenemos cientos de miles de puntos, pero muy poco prácticos para la leyenda, donde sólo hay un minúsculo punto por etiqueta. Los puntos sobre el mapa se superponen en tal medida que es difícil identificar patrones espaciales distintos según su categoría. El primer problema se resuelve fijando “a mano” los atributos de la leyenda, asi: ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud, color = tipo), size = 0.1, alpha = 0.1) + guides(color = guide_legend(override.aes = list(size = 1, alpha = 1))) El segundo, usando facetado para mostrar en su propio mapa a cada categoría: ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud, color = tipo), size = 0.1, alpha = 0.1) + guides(color = guide_legend(override.aes = list(size = 1, alpha = 1))) + facet_wrap(vars(tipo)) El facetado ayuda a que no se nos mezclen los colores, y hace evidente cuales categorías son mas frecuentes que otras. Pero con nuestra abundancia de puntos no ayuda encontrar los sitios de alta concentración, y hace que se pierdan de vista los casos de la categoría poco frecuente (homicidios). Para hacer las diferencias aún mas nítidas, podemos facetar una estimación de densidad en lugar de puntos. ¿Cómo lo haríamos? ggmap(CABA) + geom_density2d_filled(data = delitos, aes(x = longitud, y = latitud), alpha = 0.5) + facet_wrap(vars(tipo)) 6.3 Combinando espacio y tiempo El facetado también nos permite visualizar el cambio de posición a través del tiempo. Por ejemplo, podemos comparar cierto tipo delito (hurto sin violencia) mostrando dónde ocurre en cada día de la semana. Primero activamos el paquete dplyr para acceder a su función de filtrado de datos, library(dplyr) Y luego mostramos sólo las filas del dataframe donde la columna tipo contiene “Homicidio”, - en forma de puntos en el mapa (geom_point()), - con el “subtipo” de homicidio representado por el color de los puntos - y un facetado por día de la semana (facet_wrap(vars(wday(fecha, label = TRUE)))) ggmap(CABA) + geom_point(data = filter(delitos, tipo == &quot;Homicidio&quot;), aes(x = longitud, y = latitud, color = subtipo), alpha = .5) + facet_wrap(vars(wday(fecha, label = TRUE))) Vale aclarar que el poco elegante facet_wrap(vars(wday(fecha, label = TRUE))) podría cambiarse por un más legible facet_wrap(vars(dia_semana)) si como paso previo agregamos al dataframe la columna “dia_semana”, guardando allí el valor obtenido con wday(). Volviendo al tiempo y el espacio, también podemos concentrarnos en un tipo de delito en particular, y evaluar en que zonas se concentra de acuerdo a la hora del día. Nuestra data ya tiene la hora del día declarada en una columna, “franja”. Si no fuera así, y tuviéramos la hora como parte de la fecha (estilo “2020-09-18 14:00:00”) podríamos obtenerla con ayuda de hour() que funciona de forma similar a las ya vistas month() y wday(). Entonces, mostremos sólo las filas del dataframe donde la columna tipo contiene “Hurto (sin violencia)”, - en forma de mapa de densidad (geom_density2d_filled()), - y un facetado por hora del día (facet_wrap(vars(franja))) ggmap(CABA) + geom_density2d_filled(data = filter(delitos, tipo == &quot;Hurto (sin violencia)&quot;), aes(x = longitud, y = latitud), alpha = .5) + facet_wrap(vars(franja)) En el resultado se puede ver como los hurtos se concentran nítidamente en las áreas de mayor actividad comercial durante el día (lo que los porteños llaman “el centro”), sobre todo desde el mediodía hasta 4 o 5 de la tarde, cuando pierde intensidad y se dispersa en la dirección de las principales avenidas de la ciudad. Para terminar, pulimos la visualización - filtrando las filas que registran la franja horaria como desconocida (!is.na(franja)) - retirando la leyenda, ya que nos interesa mostrar como se mueve la densidad a lo largo del día más que las cantidades - eligiendo la cantidad de filas en la que se distribuirán las facetas (nrow = 4) - agregando título, subtítulo, y nota al pie con fuente - eligiendo un tema apropiado ggmap(CABA) + geom_density2d_filled(data = filter(delitos, !is.na(franja), tipo == &quot;Hurto (sin violencia)&quot;), aes(x = longitud, y = latitud), alpha = .5) + guides(fill = FALSE) + facet_wrap(vars(franja), nrow = 4) + labs(title = &quot;Ciudad de Buenos Aires: concentración espacial de hurtos&quot;, subtitle = &quot;según hora del día, durante el año 2020&quot;, caption = &quot;fuente: https://mapa.seguridadciudad.gob.ar&quot;) + theme_void() 6.4 Ejercicios I.X II.X X Con eso cerramos el capítulo, y el manual. Esperamos haber provisto una introducción satisfactoria al análisis, modelado y visualización de información, y que haya sido tan sólo el inicio de un largo y gratificante recorrido. ¡Gracias por haber llegado hasta aquí! "]]
